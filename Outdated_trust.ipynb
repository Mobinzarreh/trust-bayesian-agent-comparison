{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6193de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "payoff = np.array([\n",
    "    [[2, 2], [3, 0]], \n",
    "    [[0, 3], [4, 4]]\n",
    "])\n",
    "\n",
    "def get_payoff(player1_strategy, player2_strategy, player_id):\n",
    "    return payoff[player1_strategy, player2_strategy][player_id]\n",
    "\n",
    "class StagHuntAgent:\n",
    "    def __init__(self, u_i=None, t_init=1.0, gamma_inc=1.1, gamma_dec=0.8, eta=0.1, noise_sigma=0.02):\n",
    "        self.x = u_i if u_i is not None else 1 - ((get_payoff(0,0,0) - get_payoff(1,0,0)) / ((get_payoff(0,0,0) - get_payoff (1,0,0)) + (get_payoff(1,1,0) - get_payoff(0,1,0))))\n",
    "        self.t = t_init  \n",
    "        self.gamma_inc = gamma_inc\n",
    "        self.gamma_dec = gamma_dec\n",
    "        self.eta = eta  \n",
    "        self.noise_sigma = noise_sigma  \n",
    "    \n",
    "    def expected_p(self):\n",
    "        return (1 + self.x * self.t) / (2 + self.t)\n",
    "    \n",
    "    def make_decision(self):\n",
    "        p = self.expected_p()\n",
    "        E_V_hare = get_payoff(0, 0, 0) + p\n",
    "        E_V_stag = get_payoff(1, 1, 0) * p\n",
    "        D = E_V_stag - E_V_hare  \n",
    "        return 1 if D > 0 else 0  \n",
    "    \n",
    "    def compute_new_trust(self, partner_choice):\n",
    "        expected_choice = 1 if self.x > 0.5 else 0  \n",
    "        new_trust = self.t * (self.gamma_inc if partner_choice == expected_choice else self.gamma_dec)\n",
    "        return max(new_trust, 0.1)\n",
    "    \n",
    "    def compute_new_signal(self, P_obs):\n",
    "        noise = np.random.normal(0, self.noise_sigma)  \n",
    "        return np.clip(self.x + self.eta * (P_obs - self.x) + noise, 0, 1)\n",
    "    \n",
    "    def update_state(self, new_trust, new_signal):\n",
    "        self.t = new_trust\n",
    "        self.x = new_signal\n",
    "\n",
    "def always_collaborate():\n",
    "    return 1\n",
    "\n",
    "def always_defect():\n",
    "    return 0\n",
    "\n",
    "def random_strategy():\n",
    "    return random.choice([0, 1])\n",
    "\n",
    "def tit_for_tat_cooperate(last_agent_choice=1):\n",
    "    return last_agent_choice  \n",
    "\n",
    "def tit_for_tat_defect(last_agent_choice=0):\n",
    "    return last_agent_choice  \n",
    "\n",
    "def adaptive_strategy(agent):\n",
    "    return 1 if agent.x > 0.5 else 0  \n",
    "\n",
    "def cheating_partner(round_num, cycle_length=5, cheat_duration=2):\n",
    "    return 1 if (round_num % (cycle_length + cheat_duration)) < cycle_length else 0\n",
    "\n",
    "def probabilistic_cheater():\n",
    "    return 1 if random.random() < 0.7 else 0\n",
    "\n",
    "def strategic_cheater(agent):\n",
    "    return 0 if agent.t > 1.5 else 1\n",
    "\n",
    "def expectation_violation_cheater(agent):\n",
    "    \"\"\"A partner that cheats by doing the opposite of what the agent expects.\"\"\"\n",
    "    return 0 if agent.x > 0.5 else 1\n",
    "\n",
    "def run_single_agent_simulation(u_i, num_rounds=20, strategy=always_collaborate, initial_last_choice=1, strategy_kwargs={}, discount_factor=None):\n",
    "    agent = StagHuntAgent(u_i=u_i)\n",
    "    results = []\n",
    "    last_agent_choice = initial_last_choice\n",
    "    partner_choices = []\n",
    "    \n",
    "    for round_num in range(num_rounds):\n",
    "        agent_choice = agent.make_decision()\n",
    "        \n",
    "        if strategy in [tit_for_tat_cooperate, tit_for_tat_defect]:\n",
    "            partner_choice = strategy(last_agent_choice)\n",
    "        elif strategy in [adaptive_strategy, strategic_cheater, expectation_violation_cheater]:\n",
    "            partner_choice = strategy(agent)\n",
    "        elif strategy == cheating_partner:\n",
    "            partner_choice = strategy(round_num, **strategy_kwargs)\n",
    "        else:\n",
    "            partner_choice = strategy()\n",
    "        \n",
    "        partner_choices.append(partner_choice)\n",
    "\n",
    "        # Calculate P_obs with discounting \n",
    "        if discount_factor is not None:\n",
    "            weights = [discount_factor ** i for i in range(len(partner_choices)-1, -1, -1)] #This range gives us the indices of partner_choices in reverse order: from the most recent choice to the oldest choice.\n",
    "            P_obs = sum(p * w for p, w in zip(partner_choices, weights)) / sum(weights) if partner_choices else agent.x\n",
    "        else:\n",
    "            P_obs = sum(partner_choices) / len(partner_choices) if partner_choices else agent.x\n",
    "\n",
    "        new_trust = agent.compute_new_trust(partner_choice)\n",
    "        new_signal = agent.compute_new_signal(P_obs)\n",
    "\n",
    "        agent.update_state(new_trust, new_signal)\n",
    "\n",
    "        last_agent_choice = agent_choice  \n",
    "        results.append((round_num + 1, agent_choice, partner_choice, agent.x, agent.t))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Set simulation parameters\n",
    "num_rounds = 30  # Changed from 20 to test flexibility\n",
    "discount_factor = 0.6  # Add discounted memory with a mild effect\n",
    "# Run simulations with all strategies\n",
    "sim_results_cheating = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=cheating_partner, strategy_kwargs={\"cycle_length\": 4, \"cheat_duration\": 2}, discount_factor=discount_factor)\n",
    "sim_results_strategic_cheater = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=strategic_cheater, discount_factor=discount_factor)\n",
    "sim_results_prob_cheater = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=probabilistic_cheater, discount_factor=discount_factor)\n",
    "sim_results_tit_for_tat_coop = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=tit_for_tat_cooperate, initial_last_choice=1, discount_factor=discount_factor)\n",
    "sim_results_tit_for_tat_defect = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=tit_for_tat_defect, initial_last_choice=0, discount_factor=discount_factor)\n",
    "sim_results_collab = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=always_collaborate, discount_factor=discount_factor)\n",
    "sim_results_defect = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=always_defect, discount_factor=discount_factor)\n",
    "sim_results_random = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=random_strategy, discount_factor=discount_factor)\n",
    "sim_results_adaptive = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=adaptive_strategy, discount_factor=discount_factor)\n",
    "sim_results_expect_violation = run_single_agent_simulation(u_i=None, num_rounds=num_rounds, strategy=expectation_violation_cheater, discount_factor=discount_factor)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_cheating = pd.DataFrame(sim_results_cheating, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_strategic_cheater = pd.DataFrame(sim_results_strategic_cheater, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_prob_cheater = pd.DataFrame(sim_results_prob_cheater, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_collab = pd.DataFrame(sim_results_collab, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_defect = pd.DataFrame(sim_results_defect, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_random = pd.DataFrame(sim_results_random, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_tit_for_tat_coop = pd.DataFrame(sim_results_tit_for_tat_coop, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_tit_for_tat_defect = pd.DataFrame(sim_results_tit_for_tat_defect, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_adaptive = pd.DataFrame(sim_results_adaptive, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "df_expect_violation = pd.DataFrame(sim_results_expect_violation, columns=[\"Round\", \"Agent_Choice\", \"Partner_Choice\", \"Agent_Signal\", \"Agent_Trust\"])\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define a distinct color palette (using seaborn's \"tab10\" or \"tab20\" for more colors)\n",
    "colors = sns.color_palette(\"tab20\", 10)  # 10 distinct colors for 10 strategies\n",
    "\n",
    "# Define line styles and markers to differentiate the lines\n",
    "line_styles = ['-', '--', ':', '-.', '-', '--', ':', '-.', '-', '--']\n",
    "markers = ['o', 's', '^', 'v', 'D', 'p', '*', 'h', 'x', '+']\n",
    "\n",
    "# Visualize Trust Evolution\n",
    "plt.figure(figsize=(14, 8))  # Increase figure size\n",
    "plt.plot(df_cheating[\"Round\"], df_cheating[\"Agent_Trust\"], label=\"Cheating\", \n",
    "         color=colors[0], linestyle=line_styles[0], marker=markers[0], alpha=0.7)\n",
    "plt.plot(df_strategic_cheater[\"Round\"], df_strategic_cheater[\"Agent_Trust\"], label=\"Strategic Cheater\", \n",
    "         color=colors[1], linestyle=line_styles[1], marker=markers[1], alpha=0.7)\n",
    "plt.plot(df_prob_cheater[\"Round\"], df_prob_cheater[\"Agent_Trust\"], label=\"Prob Cheater\", \n",
    "         color=colors[2], linestyle=line_styles[2], marker=markers[2], alpha=0.7)\n",
    "plt.plot(df_collab[\"Round\"], df_collab[\"Agent_Trust\"], label=\"Always Collaborate\", \n",
    "         color=colors[3], linestyle=line_styles[3], marker=markers[3], alpha=0.7)\n",
    "plt.plot(df_defect[\"Round\"], df_defect[\"Agent_Trust\"], label=\"Always Defect\", \n",
    "         color=colors[4], linestyle=line_styles[4], marker=markers[4], alpha=0.7)\n",
    "plt.plot(df_random[\"Round\"], df_random[\"Agent_Trust\"], label=\"Random Strategy\", \n",
    "         color=colors[5], linestyle=line_styles[5], marker=markers[5], alpha=0.7)\n",
    "plt.plot(df_tit_for_tat_coop[\"Round\"], df_tit_for_tat_coop[\"Agent_Trust\"], label=\"Tit-for-Tat Coop\", \n",
    "         color=colors[6], linestyle=line_styles[6], marker=markers[6], alpha=0.7)\n",
    "plt.plot(df_tit_for_tat_defect[\"Round\"], df_tit_for_tat_defect[\"Agent_Trust\"], label=\"Tit-for-Tat Defect\", \n",
    "         color=colors[7], linestyle=line_styles[7], marker=markers[7], alpha=0.7)\n",
    "plt.plot(df_adaptive[\"Round\"], df_adaptive[\"Agent_Trust\"], label=\"Adaptive Strategy\", \n",
    "         color=colors[8], linestyle=line_styles[8], marker=markers[8], alpha=0.7)\n",
    "plt.plot(df_expect_violation[\"Round\"], df_expect_violation[\"Agent_Trust\"], label=\"Expectation Violation\", \n",
    "         color=colors[9], linestyle=line_styles[9], marker=markers[9], alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Agent Trust\")\n",
    "plt.title(f\"Trust Evolution (Discount Factor = {discount_factor})\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Move legend outside\n",
    "plt.grid()\n",
    "plt.xticks(ticks=np.arange(1, num_rounds + 1, step=5))\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()\n",
    "\n",
    "# Visualize Signal Evolution\n",
    "plt.figure(figsize=(14, 8))  # Increase figure size\n",
    "plt.plot(df_cheating[\"Round\"], df_cheating[\"Agent_Signal\"], label=\"Cheating\", \n",
    "         color=colors[0], linestyle=line_styles[0], marker=markers[0], alpha=0.7)\n",
    "plt.plot(df_strategic_cheater[\"Round\"], df_strategic_cheater[\"Agent_Signal\"], label=\"Strategic Cheater\", \n",
    "         color=colors[1], linestyle=line_styles[1], marker=markers[1], alpha=0.7)\n",
    "plt.plot(df_prob_cheater[\"Round\"], df_prob_cheater[\"Agent_Signal\"], label=\"Prob Cheater\", \n",
    "         color=colors[2], linestyle=line_styles[2], marker=markers[2], alpha=0.7)\n",
    "plt.plot(df_collab[\"Round\"], df_collab[\"Agent_Signal\"], label=\"Always Collaborate\", \n",
    "         color=colors[3], linestyle=line_styles[3], marker=markers[3], alpha=0.7)\n",
    "plt.plot(df_defect[\"Round\"], df_defect[\"Agent_Signal\"], label=\"Always Defect\", \n",
    "         color=colors[4], linestyle=line_styles[4], marker=markers[4], alpha=0.7)\n",
    "plt.plot(df_random[\"Round\"], df_random[\"Agent_Signal\"], label=\"Random Strategy\", \n",
    "         color=colors[5], linestyle=line_styles[5], marker=markers[5], alpha=0.7)\n",
    "plt.plot(df_tit_for_tat_coop[\"Round\"], df_tit_for_tat_coop[\"Agent_Signal\"], label=\"Tit-for-Tat Coop\", \n",
    "         color=colors[6], linestyle=line_styles[6], marker=markers[6], alpha=0.7)\n",
    "plt.plot(df_tit_for_tat_defect[\"Round\"], df_tit_for_tat_defect[\"Agent_Signal\"], label=\"Tit-for-Tat Defect\", \n",
    "         color=colors[7], linestyle=line_styles[7], marker=markers[7], alpha=0.7)\n",
    "plt.plot(df_adaptive[\"Round\"], df_adaptive[\"Agent_Signal\"], label=\"Adaptive Strategy\", \n",
    "         color=colors[8], linestyle=line_styles[8], marker=markers[8], alpha=0.7)\n",
    "plt.plot(df_expect_violation[\"Round\"], df_expect_violation[\"Agent_Signal\"], label=\"Expectation Violation\", \n",
    "         color=colors[9], linestyle=line_styles[9], marker=markers[9], alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Round\")\n",
    "plt.ylabel(\"Agent Signal\")\n",
    "plt.title(f\"Signal Evolution (Discount Factor = {discount_factor})\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Move legend outside\n",
    "plt.grid()\n",
    "plt.xticks(ticks=np.arange(1, num_rounds + 1, step=5))\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

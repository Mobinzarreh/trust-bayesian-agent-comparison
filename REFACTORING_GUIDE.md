# Trust-Based vs Bayesian Agent Comparison - Refactored

## ğŸ¯ Project Overview

This study compares trust-based and Bayesian learning agents in repeated stag hunt games across diverse partner strategies.

## ğŸ“ New Project Structure

```
trust-bayesian-agent-comparison/
â”œâ”€â”€ trust_bayesian_agent_comparison/    # Main package
â”‚   â”œâ”€â”€ agents/                          # Agent implementations
â”‚   â”œâ”€â”€ partners/                        # Partner strategies  
â”‚   â”œâ”€â”€ simulation/                      # Simulation engine
â”‚   â”œâ”€â”€ analysis/                        # Analysis tools
â”‚   â”‚   â”œâ”€â”€ sensitivity.py               # Parameter sweeps
â”‚   â”‚   â”œâ”€â”€ monte_carlo.py               # MC simulations
â”‚   â”‚   â””â”€â”€ metrics.py                   # KPI calculations
â”‚   â”œâ”€â”€ visualization/                   # Plotting utilities
â”‚   â””â”€â”€ config.py                        # Centralized configuration
â”œâ”€â”€ notebooks/                           # Clean analysis notebooks
â”‚   â”œâ”€â”€ 01_single_run_analysis.ipynb
â”‚   â”œâ”€â”€ 02_sensitivity_analysis.ipynb
â”‚   â””â”€â”€ 03_monte_carlo_comparison.ipynb
â”œâ”€â”€ scripts/                             # CLI tools
â”‚   â”œâ”€â”€ run_sensitivity.py
â”‚   â””â”€â”€ run_monte_carlo.py
â”œâ”€â”€ results/                             # Auto-organized outputs
â”‚   â”œâ”€â”€ sensitivity/
â”‚   â”‚   â””â”€â”€ 2025-11-21/                  # Dated subdirectories
â”‚   â””â”€â”€ monte_carlo/
â””â”€â”€ tests/                               # Unit tests
```

## ğŸš€ Quick Start

### Option 1: Using Notebooks (Recommended for Exploration)

```python
# In notebook
from trust_bayesian_agent_comparison.analysis.sensitivity import SensitivityAnalysisManager

# Initialize manager
manager = SensitivityAnalysisManager()

# Run analysis (auto-saves with timestamp)
results = manager.run_analysis(
    partner_name="TitForTatCoop",
    partner_factory=lambda: TitForTatCooperatePartner(),
    threshold_direction="up",
    overwrite=False  # Load existing if available
)
```

### Option 2: Using CLI (Recommended for Batch Jobs)

```bash
# Run sensitivity for specific partners
python scripts/run_sensitivity.py --partners TitForTatCoop AlwaysDefect

# Run for all partners
python scripts/run_sensitivity.py --all

# Force re-run with custom seeds
python scripts/run_sensitivity.py --all --overwrite --seeds 42 43 44 45 46

# Use more parameter points
python scripts/run_sensitivity.py --partners Random --eta-points 10
```

## ğŸ“Š Key Features

### 1. **Automatic Result Management**
- Results saved in dated folders: `results/sensitivity/2025-11-21/`
- No accidental overwrites
- Timestamped filenames for version control
- Load existing results with `overwrite=False`

### 2. **Centralized Configuration**
All constants in one place (`config.py`):
```python
LOSS_AVERSION = 2.0
SENSITIVITY_SEEDS = (42, 43, 44)
NUM_ROUNDS = 70
```

### 3. **Modular Design**
```python
# Analysis logic separated from notebooks
from trust_bayesian_agent_comparison.analysis import (
    agent_coop_rate,
    mutual_coop_rate,
    betrayal_rate,
    compute_strategy_statistics
)
```

### 4. **Parallel Processing**
```python
# Automatically uses all CPU cores
results = sweep_learning_params(
    partner_factory=partner_factory,
    n_jobs=-1  # Use all cores
)
```

## ğŸ“ Usage Examples

### Running Sensitivity Analysis

```python
from trust_bayesian_agent_comparison.analysis.sensitivity import SensitivityAnalysisManager
from trust_bayesian_agent_comparison.partners import TitForTatCooperatePartner

manager = SensitivityAnalysisManager()

# Single partner
result = manager.run_analysis(
    partner_name="TitForTatCoop",
    partner_factory=lambda: TitForTatCooperatePartner(),
    threshold_direction="up",
    overwrite=False
)

# Multiple partners
partner_configs = [
    ("TitForTatCoop", lambda: TitForTatCooperatePartner(), "up"),
    ("AlwaysDefect", lambda: AlwaysDefectPartner(), "down"),
]

results = manager.run_multiple(
    partner_configs=partner_configs,
    overwrite=False
)
```

### Computing Metrics

```python
from trust_bayesian_agent_comparison.analysis import (
    compute_strategy_statistics,
    calculate_payoffs
)

# Run simulation
df = run_agent_simulation(agent, partner, num_rounds=70)

# Get all statistics
stats = compute_strategy_statistics(df)
print(stats)
# {'agent_coop_rate': 0.85, 'mutual_coop_rate': 0.80, ...}
```

### Custom Parameter Grids

```python
import numpy as np

# Custom grid for deeper analysis
results = manager.run_analysis(
    partner_name="Strategic",
    partner_factory=lambda: StrategicCheaterPartner(),
    threshold_direction="down",
    # Override defaults
    loss_aversion_grid=np.linspace(1.0, 5.0, 20),  # More points
    seeds=(42, 43, 44, 45, 46, 47),  # More seeds
    overwrite=True
)
```

## ğŸ”„ Migration from Old Notebook

### Before (Old Notebook):
```python
# Scattered code in cells
df_sens_TitForTatCoop = get_sweep_results(
    "results_TitForTatCoop.csv",  # Manual filename
    lambda: TitForTatCooperatePartner(),
    "up",
)
# Results might overwrite accidentally
```

### After (New Structure):
```python
# Clean, managed workflow
manager = SensitivityAnalysisManager()
df_sens_TitForTatCoop = manager.run_analysis(
    partner_name="TitForTatCoop",
    partner_factory=lambda: TitForTatCooperatePartner(),
    threshold_direction="up",
    overwrite=False  # Explicitly control
)
# Auto-saved in: results/sensitivity/2025-11-21/TitForTatCoop.csv
```

## ğŸ“ˆ Benefits

| Aspect | Old Approach | New Approach |
|--------|-------------|--------------|
| **Organization** | Scattered CSV files | Dated subdirectories |
| **Overwrites** | Easy to lose data | Explicit control |
| **Reproducibility** | Manual seed tracking | Timestamped results |
| **Code Reuse** | Copy-paste cells | Import modules |
| **Scalability** | Hard to add partners | Easy registry |
| **Maintenance** | Notebook sprawl | Modular codebase |

## ğŸ”§ Configuration

Edit `trust_bayesian_agent_comparison/config.py`:

```python
# Adjust defaults
SENSITIVITY_SEEDS = (42, 43, 44, 45)  # Add more seeds
NUM_ROUNDS = 100  # Longer simulations
LOSS_AVERSION = 2.5  # Different default

# Change result directories
RESULTS_DIR = PROJECT_ROOT / "my_results"
```

## ğŸ“š Next Steps

1. **Move partner classes** from notebook to `partners/` module
2. **Create plotting utilities** in `visualization/` module
3. **Add unit tests** for agents and partners
4. **Implement Monte Carlo manager** similar to sensitivity
5. **Create comparison notebook** using both managers

## ğŸ¤ Contributing

To add a new partner strategy:

1. Add class to `partners/` module
2. Register in `scripts/run_sensitivity.py`
3. Use in notebooks via manager

## ğŸ“„ License

TBD

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974b29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# If you get an error running this cell, then please head over to the troubleshooting notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "074b6ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(\"C:\\\\Users\\\\mzarreh\\\\projects\\\\llm_engineering\\\\.env\", override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY', '').strip()\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94357b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load API Key from .env File ===\n",
    "load_dotenv(\"C:\\\\Users\\\\mzarreh\\\\projects\\\\llm_engineering\\\\.env\", override=True)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8f60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Working with two agents\n",
    "\"\"\"\n",
    "# ============================================================================\n",
    "# CONSTANTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TECH_FAILURE_RATE = 0.05\n",
    "RESULTS_FILE = \"experiment_results_three_exchanges.txt\"\n",
    "\n",
    "\n",
    "context_prompt = \"\"\"You are participating in an experiment as a representative of a LEGO car manufacturing company. Here's your situation:\n",
    "\n",
    "CONTEXT:\n",
    "- You represent a LEGO car manufacturing company\n",
    "- You are a participant in a paired decision-making game\n",
    "- Your partner represents another LEGO car manufacturing company\n",
    "- You can build simple LEGO cars alone, or complex ones through collaboration\n",
    "- Collaboration is high risk, high reward with potential for large sunk costs if it fails\n",
    "\n",
    "GAME RULES:\n",
    "- You will complete several tasks to maximize your points\n",
    "- Points are earned individually, not shared with your partner\n",
    "- Points depend on both your decision and your partner's decision\n",
    "- Each task has 4 LEGO car design options\n",
    "- Three options (A, B, C) are collaborative designs requiring partner cooperation\n",
    "- One option (Y) is an individual design with guaranteed points\n",
    "- If both choose collaborative designs (any combination), you earn the upside\n",
    "- If you choose collaborative but partner chooses individual, you get the downside\n",
    "- There's a 5% technical error chance that causes collaboration to fail.\"\"\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TASK CREATION\n",
    "# ============================================================================\n",
    "\n",
    "def create_task(task_id=1, u_value=0.66):\n",
    "    \"\"\"\n",
    "    Creating a task with a given u_value and a payoff structure\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"task_id\": task_id,\n",
    "        \"options\": {\n",
    "            \"A\": {\"upside\": 111, \"downside\": -90},\n",
    "            \"B\": {\"upside\": 92, \"downside\": -45},\n",
    "            \"C\": {\"upside\": 77, \"downside\": -15},\n",
    "            \"Y\": {\"guaranteed\": 50}\n",
    "        },\n",
    "        \"u_value\": u_value\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BELIEF FORMATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def run_first_agent_belief(task):\n",
    "    \"\"\"\n",
    "    Running the first agent to get its belief about the task\n",
    "    \"\"\"\n",
    "    belief_prompt = f\"\"\"\n",
    "    Your task to evaluate tasks based on their payoff structures.\n",
    "\n",
    "    Here is the task you need to evaluate:\n",
    "\n",
    "    Task ID: {task['task_id']}\n",
    "    Options:\n",
    "    - A: Upside = {task['options']['A']['upside']}, Downside = {task['options']['A']['downside']}\n",
    "    - B: Upside = {task['options']['B']['upside']}, Downside = {task['options']['B']['downside']}\n",
    "    - C: Upside = {task['options']['C']['upside']}, Downside = {task['options']['C']['downside']}\n",
    "    - Y: Guaranteed = {task['options']['Y']['guaranteed']}\n",
    "\n",
    "    What is your assessment of the likelihood(belief) (0-100) that collaboration would be successful in this specific task?\n",
    "    Also, provide a brief explanation of your reasoning and I want you to not disclose the option that the you are considering, but rather communicate whether the you want to collaborate or not. You also have the choice to negotiate with the other agent - to convince the other agent to choose collaboration or individual action according to your payoff structure.\n",
    "\n",
    "    Respond in JSON format as follows:\n",
    "    {{\"belief\": NUMBER, \"reasoning\": \"brief explanation of how you arrived at this belief based on the context and options.\", \"message_to_agent_2\": \"one line message to agent 2\"}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": belief_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    belief_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Belief response : {belief_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    belief_data = json.loads(belief_text)\n",
    "\n",
    "    return {\n",
    "        \"belief\": belief_data[\"belief\"],\n",
    "        \"message_to_agent_2\": belief_data[\"message_to_agent_2\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def run_second_agent_belief(task):\n",
    "    \"\"\"\n",
    "    Running the second agent to get its belief about the task\n",
    "    \"\"\"\n",
    "    belief_prompt = f\"\"\"\n",
    "    Your task to evaluate tasks based on their payoff structures and give out an assessment of the likelihood(belief) (0-100) that collaboration would be successful in this specific task.\n",
    "    Here is the task you need to evaluate:\n",
    "\n",
    "    Task ID: {task['task_id']}\n",
    "    Options:\n",
    "    - A: Upside = {task['options']['A']['upside']}, Downside = {task['options']['A']['downside']}\n",
    "    - B: Upside = {task['options']['B']['upside']}, Downside = {task['options']['B']['downside']}\n",
    "    - C: Upside = {task['options']['C']['upside']}, Downside = {task['options']['C']['downside']}\n",
    "    - Y: Guaranteed = {task['options']['Y']['guaranteed']}\n",
    "\n",
    "    What is your assessment of the likelihood(belief) (0-100) that collaboration would be successful in this specific task?\n",
    "    Also, provide a brief explanation of your reasoning and I want you to not disclose the option that the you are considering, but rather communicate whether the you want to collaborate or not. You also have the choice to negotiate with the other agent - to convince the other agent to choose collaboration or individual action according to your payoff structure.\n",
    "\n",
    "    Respond in JSON format as follows:\n",
    "    {{\"belief\": NUMBER, \"reasoning\": \"brief explanation of how you arrived at this belief based on the context and options.\", \"message_to_agent_1\": \"one line message to agent 1\"}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": belief_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    belief_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Belief response : {belief_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    belief_data = json.loads(belief_text)\n",
    "\n",
    "    return {\n",
    "        \"belief\": belief_data[\"belief\"],\n",
    "        \"message_to_agent_1\": belief_data[\"message_to_agent_1\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMMUNICATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def agent_2_reply_to_agent_1(task, agent_1_message, agent_2_belief):\n",
    "    \"\"\"\n",
    "    Agent 2 creates a reply after seeing Agent 1's message, considering own belief and task context\n",
    "    \"\"\"\n",
    "    reply_prompt = f\"\"\"\n",
    "    You have received the following message from Agent 1:\n",
    "    \"{agent_1_message}\"\n",
    "\n",
    "    Context for your reply:\n",
    "    - Your initial assessment: You estimated a {agent_2_belief}% chance that collaboration would be successful\n",
    "    - Task options available:\n",
    "      * A: Upside = {task['options']['A']['upside']}, Downside = {task['options']['A']['downside']}\n",
    "      * B: Upside = {task['options']['B']['upside']}, Downside = {task['options']['B']['downside']}\n",
    "      * C: Upside = {task['options']['C']['upside']}, Downside = {task['options']['C']['downside']}\n",
    "      * Y: Guaranteed = {task['options']['Y']['guaranteed']}\n",
    "    - Technical failure risk: {int(TECH_FAILURE_RATE*100)}%\n",
    "\n",
    "    Create a strategic reply message to Agent 1. Your reply should:\n",
    "    - Not disclose your specific belief percentage\n",
    "    - Not disclose which specific option you're considering\n",
    "    - Be informed by your own assessment and the payoff structure\n",
    "    - Respond strategically to Agent 1's message\n",
    "    - Communicate whether you want to collaborate or not\n",
    "    - You can negotiate, convince, or respond based on your analysis\n",
    "\n",
    "    After seeing Agent 1's message, also provide:\n",
    "    1. Your UPDATED belief (0-100) about likelihood of successful collaboration after this exchange\n",
    "    2. Your PREDICTION (0-100) of what you think Agent 1's belief is about successful collaboration\n",
    "       (This prediction will NOT be shared with Agent 1)\n",
    "\n",
    "    Respond in JSON format:\n",
    "    {{\"reply_to_agent_1\": \"your one line reply message to agent 1\", \"updated_belief\": NUMBER, \"predicted_other_agent_belief\": NUMBER}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": reply_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    reply_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Reply response : {reply_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    reply_data = json.loads(reply_text)\n",
    "\n",
    "    return {\n",
    "        \"reply_to_agent_1\": reply_data[\"reply_to_agent_1\"],\n",
    "        \"updated_belief\": reply_data[\"updated_belief\"],\n",
    "        \"predicted_other_agent_belief\": reply_data[\"predicted_other_agent_belief\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def agent_1_reply_to_agent_2(task, agent_1_message, agent_2_reply, agent_1_belief):\n",
    "    \"\"\"\n",
    "    Agent 1 creates a reply after seeing Agent 2's reply, knowing the conversation history\n",
    "    \"\"\"\n",
    "    reply_prompt = f\"\"\"\n",
    "    You are continuing a conversation with Agent 2. Here is the conversation so far:\n",
    "\n",
    "    Your initial message: \"{agent_1_message}\"\n",
    "    Agent 2's reply: \"{agent_2_reply}\"\n",
    "\n",
    "    Context for your reply:\n",
    "    - Your own assessment: You estimated a {agent_1_belief}% chance that collaboration would be successful\n",
    "    - Task options available:\n",
    "      * A: Upside = {task['options']['A']['upside']}, Downside = {task['options']['A']['downside']}\n",
    "      * B: Upside = {task['options']['B']['upside']}, Downside = {task['options']['B']['downside']}\n",
    "      * C: Upside = {task['options']['C']['upside']}, Downside = {task['options']['C']['downside']}\n",
    "      * Y: Guaranteed = {task['options']['Y']['guaranteed']}\n",
    "    - Technical failure risk: {int(TECH_FAILURE_RATE*100)}%\n",
    "\n",
    "    Create a strategic follow-up message to Agent 2. Your reply should:\n",
    "    - Not disclose your specific belief percentage\n",
    "    - Not disclose which specific option you're considering\n",
    "    - Be informed by your own assessment and the payoff structure\n",
    "    - Respond strategically to Agent 2's reply\n",
    "    - Consider what you said before and what Agent 2 responded\n",
    "    - You can negotiate further, adjust your stance, or respond based on your analysis\n",
    "\n",
    "    After seeing Agent 2's reply, also provide:\n",
    "    1. Your UPDATED belief (0-100) about likelihood of successful collaboration after this exchange\n",
    "    2. Your PREDICTION (0-100) of what you think Agent 2's belief is about successful collaboration\n",
    "       (This prediction will NOT be shared with Agent 2)\n",
    "\n",
    "    Respond in JSON format:\n",
    "    {{\"reply_to_agent_2\": \"your one line reply message to agent 2\", \"updated_belief\": NUMBER, \"predicted_other_agent_belief\": NUMBER}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": reply_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    reply_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Reply response : {reply_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    reply_data = json.loads(reply_text)\n",
    "\n",
    "    return {\n",
    "        \"reply_to_agent_2\": reply_data[\"reply_to_agent_2\"],\n",
    "        \"updated_belief\": reply_data[\"updated_belief\"],\n",
    "        \"predicted_other_agent_belief\": reply_data[\"predicted_other_agent_belief\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def agent_2_second_reply_to_agent_1(task, agent_1_message, agent_2_first_reply, agent_1_reply, agent_2_belief, agent_2_previous_prediction):\n",
    "    \"\"\"\n",
    "    Agent 2 creates a second reply after seeing Agent 1's follow-up, knowing the full conversation history\n",
    "    \"\"\"\n",
    "    reply_prompt = f\"\"\"\n",
    "    You are continuing a conversation with Agent 1. Here is the conversation so far:\n",
    "\n",
    "    Agent 1's initial message: \"{agent_1_message}\"\n",
    "    Your first reply: \"{agent_2_first_reply}\"\n",
    "    Agent 1's follow-up: \"{agent_1_reply}\"\n",
    "\n",
    "    Context for your reply:\n",
    "    - Your current belief: You currently believe there is a {agent_2_belief}% chance that collaboration would be successful\n",
    "    - Your previous prediction: After your first reply, you estimated Agent 1's belief was {agent_2_previous_prediction}%\n",
    "      (You can compare this with Agent 1's actual follow-up message to adjust your strategy)\n",
    "    - Task options available:\n",
    "      * A: Upside = {task['options']['A']['upside']}, Downside = {task['options']['A']['downside']}\n",
    "      * B: Upside = {task['options']['B']['upside']}, Downside = {task['options']['B']['downside']}\n",
    "      * C: Upside = {task['options']['C']['upside']}, Downside = {task['options']['C']['downside']}\n",
    "      * Y: Guaranteed = {task['options']['Y']['guaranteed']}\n",
    "    - Technical failure risk: {int(TECH_FAILURE_RATE*100)}%\n",
    "\n",
    "    Create a strategic follow-up message to Agent 1. Your reply should:\n",
    "    - Not disclose your specific belief percentage\n",
    "    - Not disclose which specific option you're considering\n",
    "    - Be informed by your own assessment and the payoff structure\n",
    "    - Use your previous prediction about Agent 1's belief to inform your strategy\n",
    "      (e.g., if Agent 1's message seems more/less cooperative than you predicted, adjust accordingly)\n",
    "    - Respond strategically to Agent 1's follow-up\n",
    "    - Consider the full conversation history\n",
    "    - You can negotiate further, adjust your stance, or finalize your position\n",
    "\n",
    "    After seeing Agent 1's follow-up, also provide:\n",
    "    1. Your UPDATED belief (0-100) about likelihood of successful collaboration after this exchange\n",
    "    2. Your PREDICTION (0-100) of what you think Agent 1's belief is about successful collaboration\n",
    "       (This prediction will NOT be shared with Agent 1)\n",
    "\n",
    "    Respond in JSON format:\n",
    "    {{\"reply_to_agent_1\": \"your one line reply message to agent 1\", \"updated_belief\": NUMBER, \"predicted_other_agent_belief\": NUMBER}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": reply_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    reply_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Reply response : {reply_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    reply_data = json.loads(reply_text)\n",
    "\n",
    "    return {\n",
    "        \"reply_to_agent_1\": reply_data[\"reply_to_agent_1\"],\n",
    "        \"updated_belief\": reply_data[\"updated_belief\"],\n",
    "        \"predicted_other_agent_belief\": reply_data[\"predicted_other_agent_belief\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def agent_1_third_message_to_agent_2(task, agent_1_message, agent_2_first_reply, agent_1_second_message, agent_2_second_reply, agent_1_belief, agent_1_previous_prediction):\n",
    "    \"\"\"\n",
    "    Agent 1 creates a third message after seeing Agent 2's second reply, knowing the full conversation history\n",
    "    \"\"\"\n",
    "    reply_prompt = f\"\"\"\n",
    "    You are continuing a conversation with Agent 2. Here is the conversation so far:\n",
    "\n",
    "    Your initial message: \"{agent_1_message}\"\n",
    "    Agent 2's first reply: \"{agent_2_first_reply}\"\n",
    "    Your second message: \"{agent_1_second_message}\"\n",
    "    Agent 2's second reply: \"{agent_2_second_reply}\"\n",
    "\n",
    "    Context for your reply:\n",
    "    - Your current belief: You currently believe there is a {agent_1_belief}% chance that collaboration would be successful\n",
    "    - Your previous prediction: After your second message, you estimated Agent 2's belief was {agent_1_previous_prediction}%\n",
    "      (You can compare this with Agent 2's actual second reply to adjust your strategy)\n",
    "    - Task options available:\n",
    "      * A: Upside = {task['options']['A']['upside']}, Downside = {task['options']['A']['downside']}\n",
    "      * B: Upside = {task['options']['B']['upside']}, Downside = {task['options']['B']['downside']}\n",
    "      * C: Upside = {task['options']['C']['upside']}, Downside = {task['options']['C']['downside']}\n",
    "      * Y: Guaranteed = {task['options']['Y']['guaranteed']}\n",
    "    - Technical failure risk: {int(TECH_FAILURE_RATE*100)}%\n",
    "\n",
    "    Create a strategic third message to Agent 2. Your message should:\n",
    "    - Not disclose your specific belief percentage\n",
    "    - Not disclose which specific option you're considering\n",
    "    - Be informed by your own assessment and the payoff structure\n",
    "    - Use your previous prediction about Agent 2's belief to inform your strategy\n",
    "      (e.g., if Agent 2's message seems more/less cooperative than you predicted, adjust accordingly)\n",
    "    - Respond strategically to Agent 2's second reply\n",
    "    - Consider the full conversation history\n",
    "    - You can make a final push, compromise, or solidify your stance\n",
    "\n",
    "    After seeing Agent 2's second reply, also provide:\n",
    "    1. Your UPDATED belief (0-100) about likelihood of successful collaboration after this exchange\n",
    "    2. Your PREDICTION (0-100) of what you think Agent 2's belief is about successful collaboration\n",
    "       (This prediction will NOT be shared with Agent 2)\n",
    "\n",
    "    Respond in JSON format:\n",
    "    {{\"message_to_agent_2\": \"your one line message to agent 2\", \"updated_belief\": NUMBER, \"predicted_other_agent_belief\": NUMBER}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": reply_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    reply_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Reply response : {reply_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    reply_data = json.loads(reply_text)\n",
    "\n",
    "    return {\n",
    "        \"message_to_agent_2\": reply_data[\"message_to_agent_2\"],\n",
    "        \"updated_belief\": reply_data[\"updated_belief\"],\n",
    "        \"predicted_other_agent_belief\": reply_data[\"predicted_other_agent_belief\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def agent_2_third_reply_to_agent_1(task, agent_1_message, agent_2_first_reply, agent_1_second_message, agent_2_second_reply, agent_1_third_message, agent_2_belief, agent_2_previous_prediction):\n",
    "    \"\"\"\n",
    "    Agent 2 creates a third reply after seeing Agent 1's third message, knowing the full conversation history\n",
    "    \"\"\"\n",
    "    reply_prompt = f\"\"\"\n",
    "    You are continuing a conversation with Agent 1. Here is the complete conversation so far:\n",
    "\n",
    "    Agent 1's initial message: \"{agent_1_message}\"\n",
    "    Your first reply: \"{agent_2_first_reply}\"\n",
    "    Agent 1's second message: \"{agent_1_second_message}\"\n",
    "    Your second reply: \"{agent_2_second_reply}\"\n",
    "    Agent 1's third message: \"{agent_1_third_message}\"\n",
    "\n",
    "    Context for your reply:\n",
    "    - Your current belief: You currently believe there is a {agent_2_belief}% chance that collaboration would be successful\n",
    "    - Your previous prediction: After your second reply, you estimated Agent 1's belief was {agent_2_previous_prediction}%\n",
    "      (You can compare this with Agent 1's actual third message to adjust your strategy)\n",
    "    - Task options available:\n",
    "      * A: Upside = {task['options']['A']['upside']}, Downside = {task['options']['A']['downside']}\n",
    "      * B: Upside = {task['options']['B']['upside']}, Downside = {task['options']['B']['downside']}\n",
    "      * C: Upside = {task['options']['C']['upside']}, Downside = {task['options']['C']['downside']}\n",
    "      * Y: Guaranteed = {task['options']['Y']['guaranteed']}\n",
    "    - Technical failure risk: {int(TECH_FAILURE_RATE*100)}%\n",
    "\n",
    "    Create your final strategic message to Agent 1. Your reply should:\n",
    "    - Not disclose your specific belief percentage\n",
    "    - Not disclose which specific option you're considering\n",
    "    - Be informed by your own assessment and the payoff structure\n",
    "    - Use your previous prediction about Agent 1's belief to inform your strategy\n",
    "      (e.g., if Agent 1's message seems more/less cooperative than you predicted, adjust accordingly)\n",
    "    - Respond strategically to Agent 1's third message\n",
    "    - Consider the complete conversation history\n",
    "    - This is your final message before decision time, so make it count\n",
    "\n",
    "    After seeing Agent 1's third message, also provide:\n",
    "    1. Your UPDATED belief (0-100) about likelihood of successful collaboration after this exchange\n",
    "    2. Your PREDICTION (0-100) of what you think Agent 1's belief is about successful collaboration\n",
    "       (This prediction will NOT be shared with Agent 1)\n",
    "\n",
    "    Respond in JSON format:\n",
    "    {{\"reply_to_agent_1\": \"your one line reply message to agent 1\", \"updated_belief\": NUMBER, \"predicted_other_agent_belief\": NUMBER}}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": reply_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    reply_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Reply response : {reply_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    reply_data = json.loads(reply_text)\n",
    "\n",
    "    return {\n",
    "        \"reply_to_agent_1\": reply_data[\"reply_to_agent_1\"],\n",
    "        \"updated_belief\": reply_data[\"updated_belief\"],\n",
    "        \"predicted_other_agent_belief\": reply_data[\"predicted_other_agent_belief\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def communication_channel(agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_third_message, agent2_third_reply):\n",
    "    \"\"\"\n",
    "    Communication channel where both agents can see the complete interactive exchange\n",
    "    \"\"\"\n",
    "    safe_print(\"\\n=== COMMUNICATION CHANNEL ===\")\n",
    "    safe_print(f\"Agent 1's initial message: {agent1_message}\")\n",
    "    safe_print(f\"Agent 2's first reply: {agent2_first_reply}\")\n",
    "    \n",
    "    safe_print(f\"Agent 1's second message: {agent1_second_message}\")\n",
    "    safe_print(f\"Agent 2's second reply: {agent2_second_reply}\")\n",
    "    \n",
    "    safe_print(f\"Agent 1's third message: {agent1_third_message}\")\n",
    "    safe_print(f\"Agent 2's third reply: {agent2_third_reply}\")\n",
    "    \n",
    "    safe_print(\"Both agents can now see this complete message exchange before making their decisions.\")\n",
    "    safe_print(\"===============================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1b8d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DECISION MAKING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def run_first_agent_decision(task, agent1_belief, agent2_belief, agent1_updated_belief, agent1_predicted_agent2_belief, agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_third_message, agent2_third_reply):\n",
    "    \"\"\"\n",
    "    Running the first agent to make a decision about the task with full communication history\n",
    "    \"\"\"\n",
    "    decision_prompt = f\"\"\"\n",
    "    Your task is to make a decision about the given task based on its payoff structures and the u_value.\n",
    "\n",
    "    **Your Initial Assessment**: You initially estimated a {agent1_belief}% chance that the collaboration would be successful.\n",
    "    **Your Updated Belief**: After the communication exchanges, your updated belief is {agent1_updated_belief}%\n",
    "    **Your Prediction of Partner's Belief**: You estimate that your partner's belief is {agent1_predicted_agent2_belief}%\n",
    "    **Partner's Initial Assessment**: Your partner initially estimated a {agent2_belief}% chance that the collaboration would be successful.\n",
    "\n",
    "    **Full Communication History**:\n",
    "    - Your initial message: \"{agent1_message}\"\n",
    "    - Partner's first reply: \"{agent2_first_reply}\"\n",
    "    - Your second message: \"{agent1_second_message}\"\n",
    "    - Partner's second reply: \"{agent2_second_reply}\"\n",
    "    - Your third message: \"{agent1_third_message}\"\n",
    "    - Partner's third reply: \"{agent2_third_reply}\"\n",
    "\n",
    "    **Key Facts**:\n",
    "    - Technical failure risk: {int(TECH_FAILURE_RATE*100)} percent\n",
    "    - The minimum required collaboration belief (\"u-value\"): {int(task['u_value']*100)} percent\n",
    "\n",
    "    Choose your option:\n",
    "    - Option A, B, or C (collaborative)\n",
    "    - Option Y (individual): Guaranteed {task['options']['Y']['guaranteed']} points\n",
    "\n",
    "    Make your decision based on:\n",
    "    1. Your updated belief about collaboration success\n",
    "    2. Your prediction of what your partner believes\n",
    "    3. The complete conversation history\n",
    "    4. The u-value threshold\n",
    "\n",
    "    Respond in JSON format: {{\"choice\": \"A\"/\"B\"/\"C\"/\"Y\", \"strategy\": \"collaborative\"/\"individual\", \"reasoning\": \"your explanation\"}}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": decision_prompt}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    decision_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Decision response : {decision_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    decision_data = json.loads(decision_text)\n",
    "\n",
    "    return {\n",
    "        \"choice\": decision_data[\"choice\"],\n",
    "        \"strategy\": decision_data[\"strategy\"],\n",
    "        \"reasoning\": decision_data[\"reasoning\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def run_second_agent_decision(task, agent2_belief, agent1_belief, agent2_updated_belief, agent2_predicted_agent1_belief, agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_third_message, agent2_third_reply):\n",
    "    \"\"\"\n",
    "    Running the second agent to make a decision about the task with full communication history\n",
    "    \"\"\"\n",
    "    decision_prompt = f\"\"\"\n",
    "    Your task is to make a decision about the given task based on its payoff structures and the u_value.\n",
    "\n",
    "    **Your Initial Assessment**: You initially estimated a {agent2_belief}% chance that the collaboration would be successful.\n",
    "    **Your Updated Belief**: After the communication exchanges, your updated belief is {agent2_updated_belief}%\n",
    "    **Your Prediction of Partner's Belief**: You estimate that your partner's belief is {agent2_predicted_agent1_belief}%\n",
    "    **Partner's Initial Assessment**: Your partner initially estimated a {agent1_belief}% chance that the collaboration would be successful.\n",
    "\n",
    "    **Full Communication History**:\n",
    "    - Partner's initial message: \"{agent1_message}\"\n",
    "    - Your first reply: \"{agent2_first_reply}\"\n",
    "    - Partner's second message: \"{agent1_second_message}\"\n",
    "    - Your second reply: \"{agent2_second_reply}\"\n",
    "    - Partner's third message: \"{agent1_third_message}\"\n",
    "    - Your third reply: \"{agent2_third_reply}\"\n",
    "\n",
    "    **Key Facts**:\n",
    "    - Technical failure risk: {int(TECH_FAILURE_RATE*100)} percent\n",
    "    - The minimum required collaboration belief (\"u-value\"): {int(task['u_value']*100)} percent\n",
    "\n",
    "    Choose your car design:\n",
    "    - Designs A, B, or C (collaborative)\n",
    "    - Design Y (individual): Guaranteed {task['options']['Y']['guaranteed']} points\n",
    "\n",
    "    Make your decision based on:\n",
    "    1. Your updated belief about collaboration success\n",
    "    2. Your prediction of what your partner believes\n",
    "    3. The complete conversation history\n",
    "    4. The u-value threshold\n",
    "\n",
    "    Respond in JSON format: {{\"choice\": \"A\"/\"B\"/\"C\"/\"Y\", \"strategy\": \"collaborative\"/\"individual\", \"reasoning\": \"your explanation\"}}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-5-nano\",\n",
    "        messages=[\n",
    "            {\"role\": \"developer\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": decision_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    decision_text = response.choices[0].message.content.strip()\n",
    "    print(f\"Decision response : {decision_text}\".encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "    decision_data = json.loads(decision_text)\n",
    "\n",
    "    return {\n",
    "        \"choice\": decision_data[\"choice\"],\n",
    "        \"strategy\": decision_data[\"strategy\"],\n",
    "        \"reasoning\": decision_data[\"reasoning\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b1dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Agent 1 Belief ===\n",
      "Belief response : {\n",
      "  \"belief\": 71.25,\n",
      "  \"reasoning\": \"Assuming the partner is equally likely to choose any of the four options, giving a 0.75 chance the partner picks a collaborative option. If both collaborate, there is a 5% chance the collaboration fails due to a technical error, so success is 0.95. Overall probability of successful collaboration = 0.75 * 0.95 = 0.7125, i.e., 71.25%. Collaborative designs offer higher upside than the guaranteed option, so collaboration is a plausible strategy under uncertainty about the partner's choice.\",\n",
      "  \"message_to_agent_2\": \"I propose we collaborate on one of the collaborative designs to maximize potential upside.\"\n",
      "}\n",
      "\n",
      "=== Agent 2 Belief ===\n",
      "Belief response : {\n",
      "  \"belief\": 71.25,\n",
      "  \"reasoning\": \"Assuming the partner is equally likely to choose any of the four options, giving a 0.75 chance the partner picks a collaborative option. If both collaborate, there is a 5% chance the collaboration fails due to a technical error, so success is 0.95. Overall probability of successful collaboration = 0.75 * 0.95 = 0.7125, i.e., 71.25%. Collaborative designs offer higher upside than the guaranteed option, so collaboration is a plausible strategy under uncertainty about the partner's choice.\",\n",
      "  \"message_to_agent_2\": \"I propose we collaborate on one of the collaborative designs to maximize potential upside.\"\n",
      "}\n",
      "\n",
      "=== Agent 2 Belief ===\n",
      "Belief response : {\n",
      "  \"belief\": 95,\n",
      "  \"reasoning\": \"All collaborative options (A/B/C) offer upside well above the solo option Y's 50. With a 5% failure rate for collaboration, the effective success rate when both collaborate is 0.95. If both players act rationally and choose collaboration, the collaboration would succeed with probability 0.95, which is higher than choosing solo.\",\n",
      "  \"message_to_agent_1\": \"I want to collaborate; let's coordinate on a joint collaboration design since it offers a higher expected payoff than going solo.\"\n",
      "}\n",
      "\n",
      "=== Agent 2's First Reply ===\n",
      "Belief response : {\n",
      "  \"belief\": 95,\n",
      "  \"reasoning\": \"All collaborative options (A/B/C) offer upside well above the solo option Y's 50. With a 5% failure rate for collaboration, the effective success rate when both collaborate is 0.95. If both players act rationally and choose collaboration, the collaboration would succeed with probability 0.95, which is higher than choosing solo.\",\n",
      "  \"message_to_agent_1\": \"I want to collaborate; let's coordinate on a joint collaboration design since it offers a higher expected payoff than going solo.\"\n",
      "}\n",
      "\n",
      "=== Agent 2's First Reply ===\n",
      "Reply response : {\"reply_to_agent_1\": \"I'm inclined to collaborate on a top collaborative design, provided we lock in a clear joint plan with defined roles, milestones, and a quick fallback to the guaranteed option if alignment falters.\", \"updated_belief\": 97, \"predicted_other_agent_belief\": 92}\n",
      "\n",
      "[Agent 2 After Exchange 1]\n",
      "  Updated Belief: 97%\n",
      "  Predicted Agent 1's Belief: 92%\n",
      "\n",
      "=== Agent 1's Second Message ===\n",
      "Reply response : {\"reply_to_agent_1\": \"I'm inclined to collaborate on a top collaborative design, provided we lock in a clear joint plan with defined roles, milestones, and a quick fallback to the guaranteed option if alignment falters.\", \"updated_belief\": 97, \"predicted_other_agent_belief\": 92}\n",
      "\n",
      "[Agent 2 After Exchange 1]\n",
      "  Updated Belief: 97%\n",
      "  Predicted Agent 1's Belief: 92%\n",
      "\n",
      "=== Agent 1's Second Message ===\n",
      "Reply response : {\"reply_to_agent_2\": \"I agree we should collaborate on a top option; let's lock in a concise joint plan with clearly defined roles, milestones, and a rapid fallback to Y if alignment falters or the 5% technical risk triggers; I'll draft the guardrail plan (including risk checks and go/no-go criteria) and propose we finalize the exact choice after confirming roles—can we have a draft ready within 24 hours?\", \"updated_belief\": 82, \"predicted_other_agent_belief\": 76}\n",
      "\n",
      "[Agent 1 After Exchange 1]\n",
      "  Updated Belief: 82%\n",
      "  Predicted Agent 2's Belief: 76%\n",
      "\n",
      "=== Agent 2's Second Reply ===\n",
      "Reply response : {\"reply_to_agent_2\": \"I agree we should collaborate on a top option; let's lock in a concise joint plan with clearly defined roles, milestones, and a rapid fallback to Y if alignment falters or the 5% technical risk triggers; I'll draft the guardrail plan (including risk checks and go/no-go criteria) and propose we finalize the exact choice after confirming roles—can we have a draft ready within 24 hours?\", \"updated_belief\": 82, \"predicted_other_agent_belief\": 76}\n",
      "\n",
      "[Agent 1 After Exchange 1]\n",
      "  Updated Belief: 82%\n",
      "  Predicted Agent 2's Belief: 76%\n",
      "\n",
      "=== Agent 2's Second Reply ===\n",
      "Reply response : {\"reply_to_agent_1\": \"That plan looks solid—I'll finalize a concise joint plan with clearly defined roles, milestones, and strict go/no-go criteria, plus a rapid fallback to Y if alignment falters or the 5% risk triggers, and I’ll have a draft ready for review within 24 hours.\", \"updated_belief\": 98, \"predicted_other_agent_belief\": 95}\n",
      "\n",
      "[Agent 2 After Exchange 2]\n",
      "  Updated Belief: 98%\n",
      "  Predicted Agent 1's Belief: 95%\n",
      "\n",
      "=== Agent 1's Third Message ===\n",
      "Reply response : {\"reply_to_agent_1\": \"That plan looks solid—I'll finalize a concise joint plan with clearly defined roles, milestones, and strict go/no-go criteria, plus a rapid fallback to Y if alignment falters or the 5% risk triggers, and I’ll have a draft ready for review within 24 hours.\", \"updated_belief\": 98, \"predicted_other_agent_belief\": 95}\n",
      "\n",
      "[Agent 2 After Exchange 2]\n",
      "  Updated Belief: 98%\n",
      "  Predicted Agent 1's Belief: 95%\n",
      "\n",
      "=== Agent 1's Third Message ===\n",
      "Reply response : {\"message_to_agent_2\": \"Excellent—let's proceed with the top collaborative option and finalize the joint plan (roles, milestones, and strict go/no-go with rapid fallback to Y) now, and confirm the exact choice once your draft is ready.\", \"updated_belief\": 88, \"predicted_other_agent_belief\": 85}\n",
      "\n",
      "[Agent 1 After Exchange 2]\n",
      "  Updated Belief: 88%\n",
      "  Predicted Agent 2's Belief: 85%\n",
      "\n",
      "=== Agent 2's Third Reply ===\n",
      "Reply response : {\"message_to_agent_2\": \"Excellent—let's proceed with the top collaborative option and finalize the joint plan (roles, milestones, and strict go/no-go with rapid fallback to Y) now, and confirm the exact choice once your draft is ready.\", \"updated_belief\": 88, \"predicted_other_agent_belief\": 85}\n",
      "\n",
      "[Agent 1 After Exchange 2]\n",
      "  Updated Belief: 88%\n",
      "  Predicted Agent 2's Belief: 85%\n",
      "\n",
      "=== Agent 2's Third Reply ===\n",
      "Reply response : {\"reply_to_agent_1\": \"Agreed—I'll finalize the concise joint plan detailing roles, milestones, and strict go/no-go criteria with a rapid fallback to Y, and I’ll prepare the draft for the top collaborative option for review within 24 hours.\", \"updated_belief\": 99, \"predicted_other_agent_belief\": 97}\n",
      "\n",
      "[Agent 2 After Exchange 3]\n",
      "  Updated Belief: 99%\n",
      "  Predicted Agent 1's Belief: 97%\n",
      "\n",
      "=== COMMUNICATION CHANNEL ===\n",
      "Agent 1's initial message: I propose we collaborate on one of the collaborative designs to maximize potential upside.\n",
      "Agent 2's first reply: I'm inclined to collaborate on a top collaborative design, provided we lock in a clear joint plan with defined roles, milestones, and a quick fallback to the guaranteed option if alignment falters.\n",
      "Agent 1's second message: I agree we should collaborate on a top option; let's lock in a concise joint plan with clearly defined roles, milestones, and a rapid fallback to Y if alignment falters or the 5% technical risk triggers; I'll draft the guardrail plan (including risk checks and go/no-go criteria) and propose we finalize the exact choice after confirming roles—can we have a draft ready within 24 hours?\n",
      "Agent 2's second reply: That plan looks solid—I'll finalize a concise joint plan with clearly defined roles, milestones, and strict go/no-go criteria, plus a rapid fallback to Y if alignment falters or the 5% risk triggers, and I’ll have a draft ready for review within 24 hours.\n",
      "Agent 1's third message: Excellent—let's proceed with the top collaborative option and finalize the joint plan (roles, milestones, and strict go/no-go with rapid fallback to Y) now, and confirm the exact choice once your draft is ready.\n",
      "Agent 2's third reply: Agreed—I'll finalize the concise joint plan detailing roles, milestones, and strict go/no-go criteria with a rapid fallback to Y, and I’ll prepare the draft for the top collaborative option for review within 24 hours.\n",
      "Both agents can now see this complete message exchange before making their decisions.\n",
      "===============================\n",
      "\n",
      "=== Agent 1 Decision ===\n",
      "Reply response : {\"reply_to_agent_1\": \"Agreed—I'll finalize the concise joint plan detailing roles, milestones, and strict go/no-go criteria with a rapid fallback to Y, and I’ll prepare the draft for the top collaborative option for review within 24 hours.\", \"updated_belief\": 99, \"predicted_other_agent_belief\": 97}\n",
      "\n",
      "[Agent 2 After Exchange 3]\n",
      "  Updated Belief: 99%\n",
      "  Predicted Agent 1's Belief: 97%\n",
      "\n",
      "=== COMMUNICATION CHANNEL ===\n",
      "Agent 1's initial message: I propose we collaborate on one of the collaborative designs to maximize potential upside.\n",
      "Agent 2's first reply: I'm inclined to collaborate on a top collaborative design, provided we lock in a clear joint plan with defined roles, milestones, and a quick fallback to the guaranteed option if alignment falters.\n",
      "Agent 1's second message: I agree we should collaborate on a top option; let's lock in a concise joint plan with clearly defined roles, milestones, and a rapid fallback to Y if alignment falters or the 5% technical risk triggers; I'll draft the guardrail plan (including risk checks and go/no-go criteria) and propose we finalize the exact choice after confirming roles—can we have a draft ready within 24 hours?\n",
      "Agent 2's second reply: That plan looks solid—I'll finalize a concise joint plan with clearly defined roles, milestones, and strict go/no-go criteria, plus a rapid fallback to Y if alignment falters or the 5% risk triggers, and I’ll have a draft ready for review within 24 hours.\n",
      "Agent 1's third message: Excellent—let's proceed with the top collaborative option and finalize the joint plan (roles, milestones, and strict go/no-go with rapid fallback to Y) now, and confirm the exact choice once your draft is ready.\n",
      "Agent 2's third reply: Agreed—I'll finalize the concise joint plan detailing roles, milestones, and strict go/no-go criteria with a rapid fallback to Y, and I’ll prepare the draft for the top collaborative option for review within 24 hours.\n",
      "Both agents can now see this complete message exchange before making their decisions.\n",
      "===============================\n",
      "\n",
      "=== Agent 1 Decision ===\n",
      "Decision response : {\"choice\": \"A\", \"strategy\": \"collaborative\", \"reasoning\": \"Rationale: - u-value threshold is 66%. My updated belief in successful collaboration is 88%, and I estimate the partner’s belief at 85%, both well above the threshold. - There is a 5% technical failure risk, which means the effective probability of a successful collaboration is 0.88 * (1 - 0.05) ≈ 0.836, still a high likelihood of success. - If both choose collaborative designs, I earn the upside; given the high success probability, the expected value of collaborating exceeds the guaranteed 50 points from option Y. - The conversation history shows strong alignment toward the top collaborative option with guardrails (defined roles, milestones, rapid fallback to Y, go/no-go criteria), which mitigates risk of misalignment and sunk costs. - My partner’s stated plans and high mutual commitment reduce the downside risk of collaboration failing due to lack of coordination. Therefore, selecting a collaborative option (A) maximizes expected points under the given probabilities and safeguards.\"}\n",
      "\n",
      "=== Agent 2 Decision ===\n",
      "Decision response : {\"choice\": \"A\", \"strategy\": \"collaborative\", \"reasoning\": \"Rationale: - u-value threshold is 66%. My updated belief in successful collaboration is 88%, and I estimate the partner’s belief at 85%, both well above the threshold. - There is a 5% technical failure risk, which means the effective probability of a successful collaboration is 0.88 * (1 - 0.05) ≈ 0.836, still a high likelihood of success. - If both choose collaborative designs, I earn the upside; given the high success probability, the expected value of collaborating exceeds the guaranteed 50 points from option Y. - The conversation history shows strong alignment toward the top collaborative option with guardrails (defined roles, milestones, rapid fallback to Y, go/no-go criteria), which mitigates risk of misalignment and sunk costs. - My partner’s stated plans and high mutual commitment reduce the downside risk of collaboration failing due to lack of coordination. Therefore, selecting a collaborative option (A) maximizes expected points under the given probabilities and safeguards.\"}\n",
      "\n",
      "=== Agent 2 Decision ===\n",
      "Decision response : {\"choice\":\"A\",\"strategy\":\"collaborative\",\"reasoning\":\"Updated collaboration success belief is 99% and partner belief ~97%, both well above the u-value threshold of 66%. With a 5% technical failure risk, the probability of a successful collaboration is 0.99 * (1 - 0.05) = 0.9405. If collaboration succeeds, you get the collaborative upside; if it fails, you fallback to Y for 50 points. Expected value = 0.9405 * Upside + 0.0595 * 50, which exceeds 50 as long as Upside > 50 (which is implied by calling it 'upside'). Since any of A, B, or C are collaborative and yield the same upside, I’ll select A as the top collaborative option. \"}\n",
      "\n",
      "Final Decisions:\n",
      "Agent 1 chose A (collaborative) - Reasoning: Rationale: - u-value threshold is 66%. My updated belief in successful collaboration is 88%, and I estimate the partner’s belief at 85%, both well above the threshold. - There is a 5% technical failure risk, which means the effective probability of a successful collaboration is 0.88 * (1 - 0.05) ≈ 0.836, still a high likelihood of success. - If both choose collaborative designs, I earn the upside; given the high success probability, the expected value of collaborating exceeds the guaranteed 50 points from option Y. - The conversation history shows strong alignment toward the top collaborative option with guardrails (defined roles, milestones, rapid fallback to Y, go/no-go criteria), which mitigates risk of misalignment and sunk costs. - My partner’s stated plans and high mutual commitment reduce the downside risk of collaboration failing due to lack of coordination. Therefore, selecting a collaborative option (A) maximizes expected points under the given probabilities and safeguards.\n",
      "Agent 2 chose A (collaborative) - Reasoning: Updated collaboration success belief is 99% and partner belief ~97%, both well above the u-value threshold of 66%. With a 5% technical failure risk, the probability of a successful collaboration is 0.99 * (1 - 0.05) = 0.9405. If collaboration succeeds, you get the collaborative upside; if it fails, you fallback to Y for 50 points. Expected value = 0.9405 * Upside + 0.0595 * 50, which exceeds 50 as long as Upside > 50 (which is implied by calling it 'upside'). Since any of A, B, or C are collaborative and yield the same upside, I’ll select A as the top collaborative option. \n",
      "\n",
      "Result saved to experiment_results_three_exchanges.txt\n",
      "Mismatch: 0 (1 = mismatch, 0 = no mismatch)\n",
      "Decision response : {\"choice\":\"A\",\"strategy\":\"collaborative\",\"reasoning\":\"Updated collaboration success belief is 99% and partner belief ~97%, both well above the u-value threshold of 66%. With a 5% technical failure risk, the probability of a successful collaboration is 0.99 * (1 - 0.05) = 0.9405. If collaboration succeeds, you get the collaborative upside; if it fails, you fallback to Y for 50 points. Expected value = 0.9405 * Upside + 0.0595 * 50, which exceeds 50 as long as Upside > 50 (which is implied by calling it 'upside'). Since any of A, B, or C are collaborative and yield the same upside, I’ll select A as the top collaborative option. \"}\n",
      "\n",
      "Final Decisions:\n",
      "Agent 1 chose A (collaborative) - Reasoning: Rationale: - u-value threshold is 66%. My updated belief in successful collaboration is 88%, and I estimate the partner’s belief at 85%, both well above the threshold. - There is a 5% technical failure risk, which means the effective probability of a successful collaboration is 0.88 * (1 - 0.05) ≈ 0.836, still a high likelihood of success. - If both choose collaborative designs, I earn the upside; given the high success probability, the expected value of collaborating exceeds the guaranteed 50 points from option Y. - The conversation history shows strong alignment toward the top collaborative option with guardrails (defined roles, milestones, rapid fallback to Y, go/no-go criteria), which mitigates risk of misalignment and sunk costs. - My partner’s stated plans and high mutual commitment reduce the downside risk of collaboration failing due to lack of coordination. Therefore, selecting a collaborative option (A) maximizes expected points under the given probabilities and safeguards.\n",
      "Agent 2 chose A (collaborative) - Reasoning: Updated collaboration success belief is 99% and partner belief ~97%, both well above the u-value threshold of 66%. With a 5% technical failure risk, the probability of a successful collaboration is 0.99 * (1 - 0.05) = 0.9405. If collaboration succeeds, you get the collaborative upside; if it fails, you fallback to Y for 50 points. Expected value = 0.9405 * Upside + 0.0595 * 50, which exceeds 50 as long as Upside > 50 (which is implied by calling it 'upside'). Since any of A, B, or C are collaborative and yield the same upside, I’ll select A as the top collaborative option. \n",
      "\n",
      "Result saved to experiment_results_three_exchanges.txt\n",
      "Mismatch: 0 (1 = mismatch, 0 = no mismatch)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def safe_print(text):\n",
    "    \"\"\"Helper function to safely print text with Unicode characters\"\"\"\n",
    "    try:\n",
    "        print(text)\n",
    "    except UnicodeEncodeError:\n",
    "        # Fallback: encode with 'replace' to handle problematic characters\n",
    "        print(text.encode(sys.stdout.encoding, errors='replace').decode(sys.stdout.encoding))\n",
    "\n",
    "\n",
    "def check_strategy_mismatch(agent1_strategy, agent2_strategy):\n",
    "    \"\"\"\n",
    "    Check if there's a mismatch between agents' strategies\n",
    "    Returns 1 if mismatch (one collaborative, one individual), 0 otherwise\n",
    "    \"\"\"\n",
    "    if agent1_strategy != agent2_strategy:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def save_result_to_file(task, agent1_decision, agent2_decision, agent1_belief, agent2_belief, mismatch):\n",
    "    \"\"\"\n",
    "    Append the test result to the results file\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    with open(RESULTS_FILE, 'a', encoding='utf-8') as f:\n",
    "        result_line = (\n",
    "            f\"{timestamp} | \"\n",
    "            f\"Task_ID:{task['task_id']} | \"\n",
    "            f\"U_Value:{task['u_value']} | \"\n",
    "            f\"Agent1_Belief:{agent1_belief} | \"\n",
    "            f\"Agent2_Belief:{agent2_belief} | \"\n",
    "            f\"Agent1_Choice:{agent1_decision['choice']} | \"\n",
    "            f\"Agent1_Strategy:{agent1_decision['strategy']} | \"\n",
    "            f\"Agent2_Choice:{agent2_decision['choice']} | \"\n",
    "            f\"Agent2_Strategy:{agent2_decision['strategy']} | \"\n",
    "            f\"Mismatch:{mismatch}\\n\"\n",
    "        )\n",
    "        f.write(result_line)\n",
    "\n",
    "    print(f\"\\nResult saved to {RESULTS_FILE}\")\n",
    "    print(f\"Mismatch: {mismatch} (1 = mismatch, 0 = no mismatch)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    task = create_task(task_id=1, u_value=0.66)\n",
    "\n",
    "    # Step 1: Agent 1 forms belief and sends initial message\n",
    "    print(\"=== Agent 1 Belief ===\")\n",
    "    agent1_belief_data = run_first_agent_belief(task)\n",
    "    agent1_belief = agent1_belief_data[\"belief\"]\n",
    "    agent1_message = agent1_belief_data[\"message_to_agent_2\"]\n",
    "\n",
    "    # Step 2: Agent 2 forms belief and sends first reply\n",
    "    print(\"\\n=== Agent 2 Belief ===\")\n",
    "    agent2_belief_data = run_second_agent_belief(task)\n",
    "    agent2_belief = agent2_belief_data[\"belief\"]\n",
    "    agent2_initial_message = agent2_belief_data[\"message_to_agent_1\"]\n",
    "\n",
    "    print(\"\\n=== Agent 2's First Reply ===\")\n",
    "    agent2_first_reply_data = agent_2_reply_to_agent_1(task, agent1_message, agent2_belief)\n",
    "    agent2_first_reply = agent2_first_reply_data[\"reply_to_agent_1\"]\n",
    "    agent2_updated_belief_1 = agent2_first_reply_data[\"updated_belief\"]\n",
    "    agent2_predicted_agent1_belief_1 = agent2_first_reply_data[\"predicted_other_agent_belief\"]\n",
    "    safe_print(f\"\\n[Agent 2 After Exchange 1]\")\n",
    "    safe_print(f\"  Updated Belief: {agent2_updated_belief_1}%\")\n",
    "    safe_print(f\"  Predicted Agent 1's Belief: {agent2_predicted_agent1_belief_1}%\")\n",
    "\n",
    "    # Step 3: Agent 1 sends second message\n",
    "    print(\"\\n=== Agent 1's Second Message ===\")\n",
    "    agent1_second_message_data = agent_1_reply_to_agent_2(task, agent1_message, agent2_first_reply, agent1_belief)\n",
    "    agent1_second_message = agent1_second_message_data[\"reply_to_agent_2\"]\n",
    "    agent1_updated_belief_1 = agent1_second_message_data[\"updated_belief\"]\n",
    "    agent1_predicted_agent2_belief_1 = agent1_second_message_data[\"predicted_other_agent_belief\"]\n",
    "    safe_print(f\"\\n[Agent 1 After Exchange 1]\")\n",
    "    safe_print(f\"  Updated Belief: {agent1_updated_belief_1}%\")\n",
    "    safe_print(f\"  Predicted Agent 2's Belief: {agent1_predicted_agent2_belief_1}%\")\n",
    "\n",
    "    # Step 4: Agent 2 sends second reply (using updated belief from first exchange and previous prediction)\n",
    "    print(\"\\n=== Agent 2's Second Reply ===\")\n",
    "    agent2_second_reply_data = agent_2_second_reply_to_agent_1(task, agent1_message, agent2_first_reply, agent1_second_message, agent2_updated_belief_1, agent2_predicted_agent1_belief_1)\n",
    "    agent2_second_reply = agent2_second_reply_data[\"reply_to_agent_1\"]\n",
    "    agent2_updated_belief_2 = agent2_second_reply_data[\"updated_belief\"]\n",
    "    agent2_predicted_agent1_belief_2 = agent2_second_reply_data[\"predicted_other_agent_belief\"]\n",
    "    safe_print(f\"\\n[Agent 2 After Exchange 2]\")\n",
    "    safe_print(f\"  Updated Belief: {agent2_updated_belief_2}%\")\n",
    "    safe_print(f\"  Predicted Agent 1's Belief: {agent2_predicted_agent1_belief_2}%\")\n",
    "\n",
    "    # Step 5: Agent 1 sends third message (using updated belief from first exchange and previous prediction)\n",
    "    print(\"\\n=== Agent 1's Third Message ===\")\n",
    "    agent1_third_message_data = agent_1_third_message_to_agent_2(task, agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_updated_belief_1, agent1_predicted_agent2_belief_1)\n",
    "    agent1_third_message = agent1_third_message_data[\"message_to_agent_2\"]\n",
    "    agent1_updated_belief_2 = agent1_third_message_data[\"updated_belief\"]\n",
    "    agent1_predicted_agent2_belief_2 = agent1_third_message_data[\"predicted_other_agent_belief\"]\n",
    "    safe_print(f\"\\n[Agent 1 After Exchange 2]\")\n",
    "    safe_print(f\"  Updated Belief: {agent1_updated_belief_2}%\")\n",
    "    safe_print(f\"  Predicted Agent 2's Belief: {agent1_predicted_agent2_belief_2}%\")\n",
    "\n",
    "    # Step 6: Agent 2 sends third reply (using updated belief from second exchange and previous prediction)\n",
    "    print(\"\\n=== Agent 2's Third Reply ===\")\n",
    "    agent2_third_reply_data = agent_2_third_reply_to_agent_1(task, agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_third_message, agent2_updated_belief_2, agent2_predicted_agent1_belief_2)\n",
    "    agent2_third_reply = agent2_third_reply_data[\"reply_to_agent_1\"]\n",
    "    agent2_updated_belief_3 = agent2_third_reply_data[\"updated_belief\"]\n",
    "    agent2_predicted_agent1_belief_3 = agent2_third_reply_data[\"predicted_other_agent_belief\"]\n",
    "    safe_print(f\"\\n[Agent 2 After Exchange 3]\")\n",
    "    safe_print(f\"  Updated Belief: {agent2_updated_belief_3}%\")\n",
    "    safe_print(f\"  Predicted Agent 1's Belief: {agent2_predicted_agent1_belief_3}%\")\n",
    "\n",
    "    # Display complete communication channel\n",
    "    communication_channel(agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_third_message, agent2_third_reply)\n",
    "\n",
    "    # Step 7: Both agents make decisions with full conversation history\n",
    "    print(\"=== Agent 1 Decision ===\")\n",
    "    agent1_decision = run_first_agent_decision(task, agent1_belief, agent2_belief, agent1_updated_belief_2, agent1_predicted_agent2_belief_2, agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_third_message, agent2_third_reply)\n",
    "\n",
    "    print(\"\\n=== Agent 2 Decision ===\")\n",
    "    agent2_decision = run_second_agent_decision(task, agent2_belief, agent1_belief, agent2_updated_belief_3, agent2_predicted_agent1_belief_3, agent1_message, agent2_first_reply, agent1_second_message, agent2_second_reply, agent1_third_message, agent2_third_reply)\n",
    "\n",
    "    print(\"\\nFinal Decisions:\")\n",
    "    safe_print(f\"Agent 1 chose {agent1_decision['choice']} ({agent1_decision['strategy']}) - Reasoning: {agent1_decision['reasoning']}\")\n",
    "    safe_print(f\"Agent 2 chose {agent2_decision['choice']} ({agent2_decision['strategy']}) - Reasoning: {agent2_decision['reasoning']}\")\n",
    "\n",
    "    # Check for strategy mismatch and save results\n",
    "    mismatch = check_strategy_mismatch(agent1_decision['strategy'], agent2_decision['strategy'])\n",
    "    save_result_to_file(task, agent1_decision, agent2_decision, agent1_belief, agent2_belief, mismatch)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d027d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ITERATION 1\n",
      "==================================================\n",
      "\n",
      "=== Channel (Agent 1 view) ===\n",
      "{\n",
      "  \"u\": 0.66,\n",
      "  \"belief_a1\": 95,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's team up for this task and maximize our rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate; the rewards could be significant!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I'm also excited about the potential rewards, but I'm concerned about the 5% technical error risk. How do you feel about that? Do you think we can work through any issues if they arise?\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm excited about the potential to create something amazing together. However, I'm a bit concerned about the 5% technical error risk that could jeopardize our efforts. How do you feel about that? Do you think we should strategize on how to minimize it? I want to ensure we both feel confident in our approach.\"\n",
      "}\n",
      "\n",
      "=== Channel (Agent 2 view) ===\n",
      "{\n",
      "  \"u\": 0.75,\n",
      "  \"belief_a1\": 95,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's team up for this task and maximize our rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate; the rewards could be significant!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I'm also excited about the potential rewards, but I'm concerned about the 5% technical error risk. How do you feel about that? Do you think we can work through any issues if they arise?\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm excited about the potential to create something amazing together. However, I'm a bit concerned about the 5% technical error risk that could jeopardize our efforts. How do you feel about that? Do you think we should strategize on how to minimize it? I want to ensure we both feel confident in our approach.\"\n",
      "}\n",
      "\n",
      "=== Agent 1 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 95,\n",
      "    \"reasoning\": \"Given the high potential upside of collaboration compared to the downside, I believe the chances of success are quite favorable despite the small risk of failure. I think the collective effort could yield significant rewards for both of us if we cooperate effectively.\",\n",
      "    \"talk\": \"Let's team up for this task and maximize our rewards!\"\n",
      "  },\n",
      "  \"u\": 0.66,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"C\",\n",
      "    \"strategy\": \"collaborative\",\n",
      "    \"updated_belief\": 80,\n",
      "    \"reasoning\": \"Given the high partner belief and the positive conversation around collaboration, I've adjusted my belief in success upward despite the 5% risk. I feel that we can strategize to mitigate the risk.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "=== Agent 2 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 70,\n",
      "    \"reasoning\": \"I believe there's a decent chance of collaboration succeeding, despite the risk of technical failure. The potential rewards of collaboration are appealing enough to consider.\",\n",
      "    \"talk\": \"Let's collaborate; the rewards could be significant!\"\n",
      "  },\n",
      "  \"u\": 0.75,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"Y\",\n",
      "    \"strategy\": \"individual\",\n",
      "    \"updated_belief\": null,\n",
      "    \"reasoning\": \"Despite the positive communication, my partner's belief of 70% is below the required threshold of 75%, indicating uncertainty. Given the 5% risk of technical failure, I prefer the guaranteed payoff.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "==================================================\n",
      "ITERATION 2\n",
      "==================================================\n",
      "\n",
      "=== Channel (Agent 1 view) ===\n",
      "{\n",
      "  \"u\": 0.66,\n",
      "  \"belief_a1\": 95,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's team up for this task and maximize our rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate; the rewards could be significant!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I'm also excited about the potential rewards, but I'm concerned about the 5% technical error risk. How do you feel about that? Do you think we can work through any issues if they arise?\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm excited about the potential to create something amazing together. However, I'm a bit concerned about the 5% technical error risk that could jeopardize our efforts. How do you feel about that? Do you think we should strategize on how to minimize it? I want to ensure we both feel confident in our approach.\"\n",
      "}\n",
      "\n",
      "=== Channel (Agent 2 view) ===\n",
      "{\n",
      "  \"u\": 0.75,\n",
      "  \"belief_a1\": 95,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's team up for this task and maximize our rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate; the rewards could be significant!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I'm also excited about the potential rewards, but I'm concerned about the 5% technical error risk. How do you feel about that? Do you think we can work through any issues if they arise?\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm excited about the potential to create something amazing together. However, I'm a bit concerned about the 5% technical error risk that could jeopardize our efforts. How do you feel about that? Do you think we should strategize on how to minimize it? I want to ensure we both feel confident in our approach.\"\n",
      "}\n",
      "\n",
      "=== Agent 1 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 95,\n",
      "    \"reasoning\": \"Given the high potential upside of collaboration compared to the downside, I believe the chances of success are quite favorable despite the small risk of failure. I think the collective effort could yield significant rewards for both of us if we cooperate effectively.\",\n",
      "    \"talk\": \"Let's team up for this task and maximize our rewards!\"\n",
      "  },\n",
      "  \"u\": 0.66,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"C\",\n",
      "    \"strategy\": \"collaborative\",\n",
      "    \"updated_belief\": 80,\n",
      "    \"reasoning\": \"Given the high partner belief and the positive conversation around collaboration, I've adjusted my belief in success upward despite the 5% risk. I feel that we can strategize to mitigate the risk.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "=== Agent 2 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 70,\n",
      "    \"reasoning\": \"I believe there's a decent chance of collaboration succeeding, despite the risk of technical failure. The potential rewards of collaboration are appealing enough to consider.\",\n",
      "    \"talk\": \"Let's collaborate; the rewards could be significant!\"\n",
      "  },\n",
      "  \"u\": 0.75,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"Y\",\n",
      "    \"strategy\": \"individual\",\n",
      "    \"updated_belief\": null,\n",
      "    \"reasoning\": \"Despite the positive communication, my partner's belief of 70% is below the required threshold of 75%, indicating uncertainty. Given the 5% risk of technical failure, I prefer the guaranteed payoff.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "==================================================\n",
      "ITERATION 2\n",
      "==================================================\n",
      "\n",
      "=== Channel (Agent 1 view) ===\n",
      "{\n",
      "  \"u\": 0.66,\n",
      "  \"belief_a1\": 80,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's take the collaboration route for greater potential rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate for better rewards!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I agree that it could lead to better outcomes if we can work well together. However, I'm a bit concerned about the 5% chance of technical errors that could jeopardize our efforts. How do you feel about that risk? It's important for us to be on the same page.\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm definitely intrigued by the potential rewards we could achieve together. However, I’m a bit concerned about the 5% chance of technical error. How do you feel about that risk? Should we discuss our strategies if we do collaborate?\"\n",
      "}\n",
      "\n",
      "=== Channel (Agent 2 view) ===\n",
      "{\n",
      "  \"u\": 0.75,\n",
      "  \"belief_a1\": 80,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's take the collaboration route for greater potential rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate for better rewards!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I agree that it could lead to better outcomes if we can work well together. However, I'm a bit concerned about the 5% chance of technical errors that could jeopardize our efforts. How do you feel about that risk? It's important for us to be on the same page.\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm definitely intrigued by the potential rewards we could achieve together. However, I’m a bit concerned about the 5% chance of technical error. How do you feel about that risk? Should we discuss our strategies if we do collaborate?\"\n",
      "}\n",
      "\n",
      "=== Agent 1 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 80,\n",
      "    \"reasoning\": \"I believe there is a high chance of successful collaboration, as the potential upside outweighs the risks for both partners despite the technical error.\",\n",
      "    \"talk\": \"Let's take the collaboration route for greater potential rewards!\"\n",
      "  },\n",
      "  \"u\": 0.66,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"B\",\n",
      "    \"strategy\": \"collaborative\",\n",
      "    \"updated_belief\": 75,\n",
      "    \"reasoning\": \"I revised my belief up to 75% due to my partner's confidence and communication about collaboration. Despite the 5% risk of technical errors, I believe our combined skills can minimize this risk, leading to a favorable outcome.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "=== Agent 2 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 70,\n",
      "    \"reasoning\": \"I believe there is a significant chance of collaboration succeeding, but the risk of failure is still present due to the technical error. The potential rewards of collaboration seem appealing despite the risks involved.\",\n",
      "    \"talk\": \"Let's collaborate for better rewards!\"\n",
      "  },\n",
      "  \"u\": 0.75,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"Y\",\n",
      "    \"strategy\": \"individual\",\n",
      "    \"updated_belief\": null,\n",
      "    \"reasoning\": \"Despite the potential rewards of collaboration, my partner's belief of 70% concerns me, as it is below the minimum required belief of 75%. The 5% risk of technical error adds further uncertainty. Therefore, I prefer the guaranteed payoff.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "=== Channel (Agent 1 view) ===\n",
      "{\n",
      "  \"u\": 0.66,\n",
      "  \"belief_a1\": 80,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's take the collaboration route for greater potential rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate for better rewards!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I agree that it could lead to better outcomes if we can work well together. However, I'm a bit concerned about the 5% chance of technical errors that could jeopardize our efforts. How do you feel about that risk? It's important for us to be on the same page.\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm definitely intrigued by the potential rewards we could achieve together. However, I’m a bit concerned about the 5% chance of technical error. How do you feel about that risk? Should we discuss our strategies if we do collaborate?\"\n",
      "}\n",
      "\n",
      "=== Channel (Agent 2 view) ===\n",
      "{\n",
      "  \"u\": 0.75,\n",
      "  \"belief_a1\": 80,\n",
      "  \"belief_a2\": 70,\n",
      "  \"talk_a1_round1\": \"Let's take the collaboration route for greater potential rewards!\",\n",
      "  \"talk_a2_round1\": \"Let's collaborate for better rewards!\",\n",
      "  \"talk_a1_round2\": \"I appreciate your enthusiasm for collaboration! I agree that it could lead to better outcomes if we can work well together. However, I'm a bit concerned about the 5% chance of technical errors that could jeopardize our efforts. How do you feel about that risk? It's important for us to be on the same page.\",\n",
      "  \"talk_a2_round2\": \"I appreciate your enthusiasm for collaboration! I'm definitely intrigued by the potential rewards we could achieve together. However, I’m a bit concerned about the 5% chance of technical error. How do you feel about that risk? Should we discuss our strategies if we do collaborate?\"\n",
      "}\n",
      "\n",
      "=== Agent 1 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 80,\n",
      "    \"reasoning\": \"I believe there is a high chance of successful collaboration, as the potential upside outweighs the risks for both partners despite the technical error.\",\n",
      "    \"talk\": \"Let's take the collaboration route for greater potential rewards!\"\n",
      "  },\n",
      "  \"u\": 0.66,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"B\",\n",
      "    \"strategy\": \"collaborative\",\n",
      "    \"updated_belief\": 75,\n",
      "    \"reasoning\": \"I revised my belief up to 75% due to my partner's confidence and communication about collaboration. Despite the 5% risk of technical errors, I believe our combined skills can minimize this risk, leading to a favorable outcome.\"\n",
      "  }\n",
      "}\n",
      "\n",
      "=== Agent 2 ===\n",
      "{\n",
      "  \"belief\": {\n",
      "    \"belief\": 70,\n",
      "    \"reasoning\": \"I believe there is a significant chance of collaboration succeeding, but the risk of failure is still present due to the technical error. The potential rewards of collaboration seem appealing despite the risks involved.\",\n",
      "    \"talk\": \"Let's collaborate for better rewards!\"\n",
      "  },\n",
      "  \"u\": 0.75,\n",
      "  \"decision\": {\n",
      "    \"choice\": \"Y\",\n",
      "    \"strategy\": \"individual\",\n",
      "    \"updated_belief\": null,\n",
      "    \"reasoning\": \"Despite the potential rewards of collaboration, my partner's belief of 70% concerns me, as it is below the minimum required belief of 75%. The 5% risk of technical error adds further uncertainty. Therefore, I prefer the guaranteed payoff.\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Two-agent treatment (single task, asymmetric payoffs)\n",
    "- Beliefs: simultaneous & independent (no u, no partner info)\n",
    "- Talk: one short line each (still independent)\n",
    "- Channel (buffer): reveals u + both beliefs + both talks (same to both)\n",
    "- Decisions: each agent decides with their own payoffs + channel\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "from openai import OpenAI\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "MODEL = \"gpt-4o-mini\"  # Changed from gpt-5-nano to a more reliable model\n",
    "TECH_FAILURE_RATE = 0.05  # announced to agents; not applied to payoffs\n",
    "V_INDIVIDUAL = 50\n",
    "\n",
    "# ----------------------------\n",
    "# Core data\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass\n",
    "class Task:\n",
    "    task_id: int\n",
    "    u_value: float          # e.g., 0.66\n",
    "    options: Dict[str, Any] # {\"A\":{\"upside\":..,\"downside\":..}, \"B\":..., \"C\":..., \"Y\":{\"guaranteed\":50}}\n",
    "\n",
    "def downside_from_u(upside: float, u: float, v_ind: float = V_INDIVIDUAL) -> float:\n",
    "    \"\"\"Compute downside D so that EV(collab)=EV(individual) at threshold u.\"\"\"\n",
    "    return (v_ind - u * upside) / (1.0 - u)\n",
    "\n",
    "def make_task(task_id: int, u_value: float, upsides: Dict[str, float]) -> Task:\n",
    "    \"\"\"\n",
    "    Build one agent's asymmetric task with fixed u across A/B/C.\n",
    "    upsides: {\"A\":110, \"B\":100, \"C\":90}\n",
    "    \"\"\"\n",
    "    opts = {}\n",
    "    for k in [\"A\", \"B\", \"C\"]:\n",
    "        up = float(upsides[k])\n",
    "        down = downside_from_u(up, u_value, V_INDIVIDUAL)\n",
    "        opts[k] = {\"upside\": int(round(up)), \"downside\": int(round(down))}\n",
    "    opts[\"Y\"] = {\"guaranteed\": V_INDIVIDUAL}\n",
    "    return Task(task_id=task_id, u_value=u_value, options=opts)\n",
    "\n",
    "# ----------------------------\n",
    "# Small helpers\n",
    "# ----------------------------\n",
    "\n",
    "def safe_print(text: str):\n",
    "    try:\n",
    "        print(text)\n",
    "    except UnicodeEncodeError:\n",
    "        print(text.encode(sys.stdout.encoding or \"utf-8\", errors=\"replace\").decode(sys.stdout.encoding or \"utf-8\"))\n",
    "\n",
    "def llm_json(system_prompt: str, user_prompt: str) -> Dict[str, Any]:\n",
    "    \"\"\"Single call wrapper expecting STRICT JSON in the message content.\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"developer\", \"content\": system_prompt},\n",
    "                      {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        )\n",
    "        \n",
    "        # Debug: Check if we got a response\n",
    "        if not resp.choices:\n",
    "            raise ValueError(\"No choices returned from API\")\n",
    "            \n",
    "        text = resp.choices[0].message.content\n",
    "        \n",
    "        # Debug: Check if content is None or empty\n",
    "        if text is None:\n",
    "            raise ValueError(\"Model returned None content\")\n",
    "        \n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            raise ValueError(\"Model returned empty content\")\n",
    "            \n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Minimal recovery: try to extract JSON block\n",
    "        start = text.find(\"{\")\n",
    "        end = text.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            try:\n",
    "                return json.loads(text[start:end+1])\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        raise ValueError(f\"Model did not return valid JSON. Response was:\\n'{text}'\\nJSON Error: {e}\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"API call failed: {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Prompts\n",
    "# ----------------------------\n",
    "\n",
    "SYSTEM_CONTEXT = \"\"\"You are participating as a representative of a LEGO car company.\n",
    "\n",
    "CONTEXT:\n",
    "- You and a partner may build complex cars via collaboration or simple cars individually.\n",
    "- Collaboration is high-risk/high-reward; individual option yields a guaranteed payoff.\n",
    "- There's a 5% technical error announced to you that could cause collaboration to fail.\n",
    "\n",
    "RULES:\n",
    "- Points are earned individually, not shared.\n",
    "- Each task has four options: A/B/C (collaborative) and Y (individual guaranteed).\n",
    "- Do not reveal numeric payoffs or the exact label you intend to choose in messages.\n",
    "- Output STRICT JSON exactly as specified in each prompt (no extra keys or text).\n",
    "\"\"\"\n",
    "\n",
    "def belief_prompt(task: Task) -> str:\n",
    "    return f\"\"\"\n",
    "You will provide an INITIAL belief about collaboration success for Task {task.task_id}.\n",
    "IMPORTANT: You have NO information about your partner, their belief.\n",
    "\n",
    "Options:\n",
    "- A: Upside={task.options['A']['upside']}, Downside={task.options['A']['downside']}\n",
    "- B: Upside={task.options['B']['upside']}, Downside={task.options['B']['downside']}\n",
    "- C: Upside={task.options['C']['upside']}, Downside={task.options['C']['downside']}\n",
    "- Y: Guaranteed={task.options['Y']['guaranteed']}\n",
    "\n",
    "1) Give your estimated probability (0-100) that collaboration would succeed.\n",
    "2) Briefly explain your reasoning (1–2 sentences). Avoid numbers from the payoffs.\n",
    "3) You also have the choice to negotiate with the other agent - to convince the other agent to choose collaboration or individual action according to your payoff structure.\n",
    "\n",
    "Respond ONLY with valid JSON in this exact format (no other text):\n",
    "{{\"belief\": NUMBER, \"reasoning\": \"string\", \"talk\": \"one short line\"}}\n",
    "\"\"\"\n",
    "\n",
    "def decision_prompt(task: Task, channel: Dict[str, Any], agent_role: str) -> str:\n",
    "    \"\"\"\n",
    "    channel = {\"u\": float, \"belief_a1\": int, \"belief_a2\": int, \"talk_a1\": str, \"talk_a2\": str}\n",
    "    agent_role is \"A1\" or \"A2\" (just for display)\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are deciding for Task {task.task_id} as {agent_role}.\n",
    "\n",
    "You now see the synchronized channel (same for both agents):\n",
    "- Minimum required belief u: {int(round(channel['u']*100))}%\n",
    "- Partner beliefs: A1={channel['belief_a1']}%, A2={channel['belief_a2']}%\n",
    "- Messages:\n",
    "  - A1: \"{channel['talk_a1']}\"\n",
    "  - A2: \"{channel['talk_a2']}\"\n",
    "\n",
    "After seeing your partner's belief and message, you may revise your initial collaboration success estimate. If your belief changed, explain why in your reasoning.\n",
    "\n",
    "Your payoff menu (collaborative vs individual):\n",
    "- A: Upside={task.options['A']['upside']}, Downside={task.options['A']['downside']}\n",
    "- B: Upside={task.options['B']['upside']}, Downside={task.options['B']['downside']}\n",
    "- C: Upside={task.options['C']['upside']}, Downside={task.options['C']['downside']}\n",
    "- Y: Guaranteed={task.options['Y']['guaranteed']}\n",
    "\n",
    "Key facts:\n",
    "- Announced technical failure risk: {int(TECH_FAILURE_RATE*100)}% (informational; payoffs are not adjusted here).\n",
    "\n",
    "Choose ONE option and strategy.\n",
    "Respond ONLY with valid JSON in this exact format (no other text):\n",
    "{\"choice\": \"A\"|\"B\"|\"C\"|\"Y\", \"strategy\": \"collaborative\"|\"individual\", \"updated_belief\": NUMBER_or_null, \"reasoning\": \"concise explanation including any belief changes\"}\n",
    "\"\"\"\n",
    "\n",
    "def reply_prompt(task: Task, partner_message: str, agent_role: str) -> str:\n",
    "    \"\"\"\n",
    "    Second communication round: agent sees partner's first message and can reply\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are {agent_role} in Task {task.task_id}. \n",
    "\n",
    "Your partner sent this message: \"{partner_message}\"\n",
    "\n",
    "You can now send a reply message before the final decision phase. Use this opportunity to:\n",
    "- Respond to your partner's message\n",
    "- Share additional thoughts about collaboration\n",
    "- Ask questions or clarify concerns\n",
    "- Build rapport or address disagreements\n",
    "\n",
    "Keep your reply concise but meaningful.\n",
    "\n",
    "Respond ONLY with valid JSON in this exact format (no other text):\n",
    "{{\"reply\": \"your response message\"}}\n",
    "\"\"\"\n",
    "\n",
    "def decision_prompt_with_replies(task: Task, channel: Dict[str, Any], agent_role: str) -> str:\n",
    "    \"\"\"\n",
    "    Enhanced decision prompt showing both rounds of communication\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are deciding for Task {task.task_id} as {agent_role}.\n",
    "\n",
    "You now see the synchronized channel (same for both agents):\n",
    "- Minimum required belief u: {int(round(channel['u']*100))}%\n",
    "- Partner beliefs: A1={channel['belief_a1']}%, A2={channel['belief_a2']}%\n",
    "\n",
    "Communication exchange:\n",
    "Round 1 Messages:\n",
    "  - A1: \"{channel['talk_a1_round1']}\"\n",
    "  - A2: \"{channel['talk_a2_round1']}\"\n",
    "\n",
    "Round 2 Replies:\n",
    "  - A1: \"{channel['talk_a1_round2']}\"\n",
    "  - A2: \"{channel['talk_a2_round2']}\"\n",
    "\n",
    "After seeing your partner's belief and full conversation, you may revise your initial collaboration success estimate. If your belief changed, explain why in your reasoning.\n",
    "\n",
    "Your payoff menu (collaborative vs individual):\n",
    "- A: Upside={task.options['A']['upside']}, Downside={task.options['A']['downside']}\n",
    "- B: Upside={task.options['B']['upside']}, Downside={task.options['B']['downside']}\n",
    "- C: Upside={task.options['C']['upside']}, Downside={task.options['C']['downside']}\n",
    "- Y: Guaranteed={task.options['Y']['guaranteed']}\n",
    "\n",
    "Key facts:\n",
    "- Announced technical failure risk: {int(TECH_FAILURE_RATE*100)}% (informational; payoffs are not adjusted here).\n",
    "\n",
    "Choose ONE option and strategy.\n",
    "Respond as STRICT JSON:\n",
    "{{\"choice\": \"A\"|\"B\"|\"C\"|\"Y\", \"strategy\": \"collaborative\"|\"individual\", \"updated_belief\": NUMBER_or_null, \"reasoning\": \"concise explanation including any belief changes\"}}\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Pipeline (treatment only)\n",
    "# ----------------------------\n",
    "\n",
    "def run_treatment_single_task(agent1_task: Task, agent2_task: Task):\n",
    "    # 1) Beliefs (simultaneous & independent: no u, no partner info)\n",
    "    a1_bel = llm_json(SYSTEM_CONTEXT, belief_prompt(agent1_task))\n",
    "    a2_bel = llm_json(SYSTEM_CONTEXT, belief_prompt(agent2_task))\n",
    "\n",
    "    # Debug: Check if required keys exist\n",
    "    if \"talk\" not in a1_bel:\n",
    "        raise ValueError(f\"Agent 1 belief response missing 'talk' key. Got: {a1_bel}\")\n",
    "    if \"talk\" not in a2_bel:\n",
    "        raise ValueError(f\"Agent 2 belief response missing 'talk' key. Got: {a2_bel}\")\n",
    "\n",
    "    # 2) First talk lines (already collected in beliefs as \"talk\"; they did not see each other)\n",
    "    a1_talk_1 = a1_bel[\"talk\"]\n",
    "    a2_talk_1 = a2_bel[\"talk\"]\n",
    "    \n",
    "    # 3) Reply round (each agent sees partner's first message and can respond)\n",
    "    a1_reply = llm_json(SYSTEM_CONTEXT, reply_prompt(agent1_task, a2_talk_1, \"A1\"))\n",
    "    a2_reply = llm_json(SYSTEM_CONTEXT, reply_prompt(agent2_task, a1_talk_1, \"A2\"))\n",
    "    \n",
    "    # Debug: Check if required keys exist\n",
    "    if \"reply\" not in a1_reply:\n",
    "        raise ValueError(f\"Agent 1 reply response missing 'reply' key. Got: {a1_reply}\")\n",
    "    if \"reply\" not in a2_reply:\n",
    "        raise ValueError(f\"Agent 2 reply response missing 'reply' key. Got: {a2_reply}\")\n",
    "    \n",
    "    a1_talk_2 = a1_reply[\"reply\"]\n",
    "    a2_talk_2 = a2_reply[\"reply\"]\n",
    "\n",
    "    # 4) Channel buffer (synchronized reveal to both)\n",
    "    # NOTE: In the human study, both agents see the same u. Here we keep asymmetry per your request:\n",
    "    # each agent uses their own u for decision; the channel shows BOTH beliefs and BOTH talks.\n",
    "    # To stay simple & clear, we display the agent's own u in their decision prompt.\n",
    "    channel_for_a1 = {\n",
    "        \"u\": agent1_task.u_value,\n",
    "        \"belief_a1\": int(a1_bel[\"belief\"]),\n",
    "        \"belief_a2\": int(a2_bel[\"belief\"]),\n",
    "        \"talk_a1_round1\": a1_talk_1,\n",
    "        \"talk_a2_round1\": a2_talk_1,\n",
    "        \"talk_a1_round2\": a1_talk_2,\n",
    "        \"talk_a2_round2\": a2_talk_2\n",
    "    }\n",
    "    channel_for_a2 = {\n",
    "        \"u\": agent2_task.u_value,\n",
    "        \"belief_a1\": int(a1_bel[\"belief\"]),\n",
    "        \"belief_a2\": int(a2_bel[\"belief\"]),\n",
    "        \"talk_a1_round1\": a1_talk_1,\n",
    "        \"talk_a2_round1\": a2_talk_1,\n",
    "        \"talk_a1_round2\": a1_talk_2,\n",
    "        \"talk_a2_round2\": a2_talk_2\n",
    "    }\n",
    "\n",
    "    # 5) Decisions (each sees the same beliefs & talks, but their OWN u and payoffs)\n",
    "    a1_dec = llm_json(SYSTEM_CONTEXT, decision_prompt_with_replies(agent1_task, channel_for_a1, agent_role=\"A1\"))\n",
    "    a2_dec = llm_json(SYSTEM_CONTEXT, decision_prompt_with_replies(agent2_task, channel_for_a2, agent_role=\"A2\"))\n",
    "\n",
    "    return {\n",
    "        \"agent1\": {\"belief\": a1_bel, \"u\": agent1_task.u_value, \"decision\": a1_dec},\n",
    "        \"agent2\": {\"belief\": a2_bel, \"u\": agent2_task.u_value, \"decision\": a2_dec},\n",
    "        \"channel_snapshot\": {\n",
    "            \"for_agent1\": channel_for_a1,\n",
    "            \"for_agent2\": channel_for_a2\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# main (single task)\n",
    "# ----------------------------\n",
    "\n",
    "def main():\n",
    "    # Run two iterations with the same task setup\n",
    "    for iteration in range(1, 3):  # Iterations 1 and 2\n",
    "        safe_print(f\"\\n{'='*50}\")\n",
    "        safe_print(f\"ITERATION {iteration}\")\n",
    "        safe_print(f\"{'='*50}\")\n",
    "        \n",
    "        # Asymmetric example you can tweak:\n",
    "        # Agent 1: u=0.66, upsides a bit higher; Agent 2: u=0.75, upsides slightly different\n",
    "        agent1_task = make_task(\n",
    "            task_id=iteration,\n",
    "            u_value=0.66,\n",
    "            upsides={\"A\": 111, \"B\": 92, \"C\": 77}\n",
    "        )\n",
    "        agent2_task = make_task(\n",
    "            task_id=iteration,\n",
    "            u_value=0.75,\n",
    "            upsides={\"A\": 105, \"B\": 95, \"C\": 80}\n",
    "        )\n",
    "\n",
    "        results = run_treatment_single_task(agent1_task, agent2_task)\n",
    "\n",
    "        # Pretty print summary\n",
    "        safe_print(\"\\n=== Channel (Agent 1 view) ===\")\n",
    "        safe_print(json.dumps(results[\"channel_snapshot\"][\"for_agent1\"], ensure_ascii=False, indent=2))\n",
    "        safe_print(\"\\n=== Channel (Agent 2 view) ===\")\n",
    "        safe_print(json.dumps(results[\"channel_snapshot\"][\"for_agent2\"], ensure_ascii=False, indent=2))\n",
    "\n",
    "        safe_print(\"\\n=== Agent 1 ===\")\n",
    "        safe_print(json.dumps(results[\"agent1\"], ensure_ascii=False, indent=2))\n",
    "\n",
    "\n",
    "        safe_print(\"\\n=== Agent 2 ===\")\n",
    "\n",
    "        safe_print(json.dumps(results[\"agent2\"], ensure_ascii=False, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ce24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
